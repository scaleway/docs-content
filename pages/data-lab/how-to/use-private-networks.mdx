---
title: How to use Private Networks with your Data Lab cluster
description: This page explains how to use Private Networks with Scaleway Data Lab for Apache Spark™
tags: private-networks private networks data lab spark apache cluster vpc
dates:
  validation: 2025-06-25
  posted: 2021-06-25
---
import Requirements from '@macros/iam/requirements.mdx'


[Private Networks](/vpc/concepts/#private-networks) allow your Data Lab for Apache Spark™ cluster to communicate in an isolated and secure network without needing to be connected to the public internet.

At the moment, Data Lab clusters can only be attached to a Private Network [during their creation](/data-lab/how-to/create-data-lab/), and cannot be detached and reattached to another Private Network afterward.

For full information about Scaleway Private Networks and VPC, see our [dedicated documentation](/vpc/) and [best practices guide](/vpc/reference-content/getting-most-private-networks/).

<Requirements />

- A Scaleway account logged into the [console](https://console.scaleway.com)
- [Owner](/iam/concepts/#owner) status or [IAM permissions](/iam/concepts/#permission) allowing you to perform actions in the intended Organization
- [Created a Private Network](/vpc/how-to/create-private-network/)
- [Created an Instance](/instances/how-to/create-an-instance/) attached to a [Private Network](/instances/how-to/use-private-networks/)

## How to use a cluster through a Private Network  

### Setting up your Instance

1. [Connect to your Instance via SSH](/instances/how-to/connect-to-instance/).

2. Run the command below from the shell of your Instance to install the required dependencies:

    ```bash
    sudo apt update
    sudo apt install -y \
      build-essential curl git \
      libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev \
      libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev \
      openjdk-17-jre-headless tmux
    ```

2. Run the command below to install `pyenv`:

    ```bash
    curl https://pyenv.run | bash
    ```

3. Run the command below to add `pyenv` to your Bash configuration:

    ```bash
    echo 'export PATH="$HOME/.pyenv/bin:$PATH"' >> ~/.bashrc
    echo 'eval "$(pyenv init -)"' >> ~/.bashrc
    echo 'eval "$(pyenv virtualenv-init -)"' >> ~/.bashrc
    ```

4. Run the command below to reload your shell:

    ```bash
    exec $SHELL
    ```

5. Run the command below to install **Python 3.13**, and activate a virtual environment:

    ```bash
    pyenv install 3.13.0
    pyenv virtualenv 3.13.0 jupyter-spark-3.13
    pyenv activate jupyter-spark-3.13
    ```

    <Message type="note">
    Your Instance Python version must be 3.13. If you encounter an error due to a mismatch between the worker and driver Python versions, run the following command to display minor versions, then reinstall using the exact one:

    ```bash
    pyenv install -l | grep 3.13
    ```
    </Message>

6. Run the command below to install JupyterLab and PySpark inside the virtual environment:

    ```bash
    pip install --upgrade pip
    pip install jupyterlab pyspark
    ```

7. Run the command below to generate a configuration file for your JupyterLab:

    ```bash
    jupyter lab --generate-config
    ```

8. Open the configuration file you just created:

    ```bash
    nano ~/.jupyter/jupyter_lab_config.py
    ```

9. Set the following parameters:

    ```python
    c.ServerApp.ip = "127.0.0.1"      # bind only to localhost
    c.ServerApp.port = 8888
    c.ServerApp.open_browser = False
    # optional authentication token:
    # c.ServerApp.token = "your-super-secure-password"
    # if running as root:
    # c.ServerApp.allow_root = True
    ```

10. Run the command below to start Jupyterlab:

    ```bash
    jupyter lab
    ```

11. Connect to your JupyterLab via SSH:

    ```bash
    ssh -L 8888:127.0.0.1:8888 <user>@<instance-public-ip>
    ```

12. Access [http://localhost:8888](http://localhost:8888)

You now have access to your Data Lab for Apache Spark™ cluster via a Private Network, using a JupyterLab notebook deployed on an Instance.

### Running a sample workload over Private Networks

1. In a new Jupyter notebook file, add the code below to a new cell:

    ```python
        from pyspark.sql import SparkSession

        MASTER_URL = "<SPARK_MASTER_HOSTNAME>" # "spark://master-datalab-[...]:7077" format
        DRIVER_HOST = "<INSTANCE_PN_IP>" # "XX.XX.XX.XX" format

        spark = (
            SparkSession.builder
            .appName("jupyter-from-vpc-instance-test")
            .master(MASTER_URL)
            # make sure executors can talk back to this driver
            .config("spark.driver.host", DRIVER_HOST)
            .config("spark.driver.bindAddress", "0.0.0.0")
            .config("spark.driver.port", "7078")
            .config("spark.blockManager.port", "7079")
            .config("spark.ui.port", "4040")
            .getOrCreate()
        )

        spark.range(10).show()
    ```

2. Replace the placeholders with the appropriate values:
    - `<SPARK_MASTER_PRIVATE_ENDPOINT>` can be found in the **Overview** tab of your cluster, under **Private endpoint** in the **Network** section.
    - `<INSTANCE_PN_IP>` can be found in the **Private Networks** tab of your Instance. Make sure to only copy the IP, and not the `/22` part.

3. Run the cell.

Your notebook hosted on an Instance is ready to be used over Private Network.

### Running an application over Private Networks using spark-submit

