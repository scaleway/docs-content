---
title: How to access the GPU using Docker
description: This page explains how to access the GPU using Docker
dates:
  validation: 2021-10-13
  posted: 2021-10-13
---

Docker is a platform as a service (PaaS) tool, that use OS-level virtualization to deliver software in packages called containers.
Each container is isolated from the others and each of them bundles their own software, libraries, and configuration files. Unlike virtual machines, containers share the services of a single operating system kernel. This reduces unnecessary overhead and makes them lightweight and portable.
Docker containers can run on any computer running macOS, Windows, or Linux, either on-premises or in a public cloud environment, such as Scaleway Elements.

All [Scaleway GPU Instances](https://www.scaleway.com/en/gpu-instances/) come with prebuilt AI Docker images, using an environment based on Pipenv. 
As mentioned earlier, Docker images are portable and you can also run images provided by other sources and use them with the GPU (for instance Docker images from Nvidia, Google, etc.).

<Message type="requirement">

- You have an account and are logged into the [Scaleway Console](https://console.scaleway.com) 
- You have created a [GPU Instance](https://www.scaleway.com/en/gpu-instances/)
- You have [generated your SSH key](https://www.scaleway.com/en/docs/console/my-project/how-to/create-ssh-key/)

</Message>

## How to get started on a GPU Instance with Docker

1. [Connect](/compute/instances/how-to/connect-to-instance/) to your Instance via SSH
2. Choose a Docker container from the containers shipped with your GPU Instance. [More information](LINK) about the available Docker containers.
3. Use the following command to pull/run the docker container:
  ```
  <insert command here>
  ```

<Message type="tip">

Refer to the [pipenv documentation](LINK) for how to use the pipenv virtual environment inside the container.

</Message>

## How to map volumes

It is recommended to map volumes into your Docker containers. You can either map local volumes on your Instance or add block storage volumes to your Instance for increased data redundancy.

### How to map local volumes

You can map local storage of your GPU Instance into your container using the `-v <local_storage>:<container_mountpoint>` flag. Following an example how to map the directory `/root/mydata` into the directory `/workspace` in the container: 

```
docker run --runtime=nvidia -it --name resnet_P100 -p 8888:8888 -p 6006:6006 -v /root/mydata/:/workspace nvcr.io/nvidia/tensorflow:20.11-tf2-py3 /bin/bash
```

### How to map Block Storage volumes 

It is also possible to map [Block Storage](https://www.scaleway.com/en/block-storage/) volumes into your containers. Block Storage is fully backed by SSDs for both classes. These three-time replacted, high-speed drives allow up to 5,000 IOPS. 
Once [attached](/storage/block/how-to/attach-a-volume/) and [mounted](/storage/block/how-to/mount-and-use-volume/) in the host OS of the GPU Instance, you can map the volume like a local volume. 

<Message type="tip">

Block Storage volumes are independent from your GPU Instance and provide three-time replicated storage. It is recommended to use Block Storage for storing your datasets, training logs, model source code, etc...

</Message>

## Most common Docker commands 

Below you find a list of the most common commands you use when dealing with Docker containers:

| Command       | Usage                                                        | Description                                                                                                                                              |
|---------------|--------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
| docker login  | `docker login`                                               | This command is used to login to Docker's default repository (Docker Hub)                                                                                |
| docker pull   | `docker pull <docker_image>`                                 | This command is used to pull images from the Docker Hub.                                                                                                 |
| docker run    | `docker run -it -d <docker_image>`                           | This command is used to create a container from an image.                                                                                                |
| docker ps     | `docker ps`                                                  | This command is used to list all running containers.                                                                                                     |
| docker ps     | `docker ps -a`                                               | This command is used to display all running and exited containers.                                                                                       |
| docker exec   | `docker exec -it <container_id> bash`                        | This command is used to access the `bash` command prompt on a running container with the ID `<container_id>`.                                            |
| docker stop   | `docker stop <container_id>`                                 | This command is used to stop a running container with the ID `<container_id>`. This command allows the container to shut down gracefully.                |
| docker kill   | `docker kill <container_id>`                                 | This command is used to "kill" a running container with the ID `<container_id>`. This commands ends the ends the execution of the container immediately. |
| docker build  | `docker build <path_to_Dockerfile>`                          | This command is used to build a new image from a specified Dockerfile.                                                                                   |
| docker commit | `docker commit <conatainer_id> <registry_user/docker_image>` | This command is used to create a new local image of an edited container.                                                                                 |
| docker push   | `docker push <registry_user/docker_image>`                   | This command is used to push a local image to a remote repository.                                                                                       |
| docker images | `docker images`                                              | This command is used to list all available docker images on the local system.                                                                            |
| docker rm     | `docker rm <container_id>`                                   | This command is used to remove a stopped container from the local system.                                                                                |
| docker rmi    | `docker rmi <image_id>`                                      | This command is used to delete an image from the local storage.                                                                                          |
| docker version| `docker --version`                                           | This command is used to display information about the currently installed version of Docker.                                                             |

For more information regarding the `docker run` command, refer to the [official documentation](https://docs.docker.com/engine/reference/run/).

## How to map ports using Docker

The default behaviour of Docker when running a container using `docker run`, is not publish any internal ports of the container to the outside world. To access services on a container outside of docker, you have to map the containers internal ports using the `--publish` or `-p` flag.

| Flag value                      | Description                                                                                                                                                |
|---------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `-p 8080:80`                    | This flag maps TCP port 80 in the container to port 8080 on the Docker host.                                                                               |
| `-p 192.168.1.100:8080:80`      | This flag maps TCP port 80 in the container to port 8080 on the Docker host for connections to host IP 192.168.1.100.                                      |
| `-p 8080:80/udp`                | This flag maps UDP port 80 in the container to port 8080 on the Docker host.                                                                               |
| `-p 8080:80/tcp -p 8080:80/udp` | This flag maps TCP port 80 in the container to TCP port 8080 on the Docker host, and map UDP port 80 in the container to UDP port 8080 on the Docker host. |


## How to access the GPU from the inside of a Docker container

You can access the GPU of your Instance from the inside of a Docker container using the `nvidia-docker` tool. 
GPUs can be specified to the Docker CLI using either the `--gpus` option (starting with Docker 19.03) or using the environment variable `NVIDIA_VISIBLE_DEVICES`, which controls which GPUs will be made accessible inside the container.

The possible values of the `NVIDIA_VISIBLE_DEVICES` variable are:

| Possible values                | Description                                                                                                   |
|--------------------------------|---------------------------------------------------------------------------------------------------------------|
| `0`,`1`,`2`, or `GPU-fef8089b` | a comma-separated list of GPU UUID(s) or index(es).                                                           |
| `all`                          | all GPUs will be accessible, this is the default value in base CUDA container images.                         |
| `none`                         | no GPU will be accessible, but driver capabilities will be enabled.                                           |
| `void or empty or unset`       | nvidia-container-runtime will have the same behavior as runc (i.e. neither GPUs nor capabilities are exposed). |

<Message type="note">

When using the `--gpus` option to specify the GPUs, the device parameter should be used. This is shown in the examples below. The format of the device parameter should be encapsulated within single quotes, followed by double quotes for the devices you want enumerated to the container. For example: `'"device=2,3"'` will enumerate GPUs 2 and 3 to the container.

When using the `NVIDIA_VISIBLE_DEVICES` variable, you may need to set `--runtime` to `nvidia`, unless already set as default.

</Message>

### Example commands 

* Starting a GPU enabled CUDA container (using `--gpus`)
  ```sh
  docker run --rm --gpus all nvidia/cuda nvidia-smi
  ```

* Starting a GPU enabled container using `NVIDIA_VISIBLE_DEVICES` and specify the nvidia runtime
  ```
  docker run --rm --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all nvidia/cuda nvidia-smi
  ```

* Starting a GPU enabled [Tensorflow](https://www.tensorflow.org/) container with a Jupyter notebook using `NVIDIA_VISIBLE_DEVICES` and map the port `88888` to access the web GUI: 
  ```
  docker run --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all -it --rm -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter
  ```

* Query the GPU UUID using nvidia-smi and then specify that to the container
  ```
  nvidia-smi -i 3 --query-gpu=uuid --format=csv
  uuid
  GPU-18a3e86f-4c0e-cd9f-59c3-55488c4b0c24

  docker run --gpus device=GPU-18a3e86f-4c0e-cd9f-59c3-55488c4b0c24 nvidia/cuda nvidia-smi
  ```

## How to push and list Docker Images stored on a Scaleway Registry

<Message type="tip">

GPU Instances are compatible with [Scaleway Container Registry](https://www.scaleway.com/en/docs/compute/container-registry/quickstart/). You can use this registry to store and pull your own docker Images.

</Message>

See [detailed information](https://www.scaleway.com/en/docs/compute/container-registry/how-to/pull-images/) on how to pull custom Docker Images on your GPU Instance.