---
meta:
  title: How to query text models
  description: Learn how to interact with powerful text models using Scaleway's Generative API service.
content:
  h1: How to query text models
  paragraph: Learn how to interact with powerful text models using Scaleway's Generative API service.
tags: generative-apis ai-data text-models
dates:
  validation: 2024-08-28
  posted: 2024-08-28
---

Scaleway's Generative AI service allows users to interact with powerful text models hosted on the platform.
The service leverages the OpenAI SDK to enable communication with the models, providing an easy-to-use interface for generating and manipulating text based on user input.
The Scaleway console provides a complete playground, where you can test models, adapt parameters, and observe how these changes affect the output in real time.

<Macro id="requirements" />

- A Scaleway account logged into the [console](https://console.scaleway.com)
- [Owner](/identity-and-access-management/iam/concepts/#owner) status or [IAM permissions](/identity-and-access-management/iam/concepts/#permission) allowing you to perform actions in the intended Organization
- A valid [API key](/identity-and-access-management/iam/how-to/create-api-keys/) (Necessary for API authentication)
- Python 3.7+ installed on your system.

## Accessing the Playground

The Playground within the Scaleway console offers an intuitive interface for experimenting with text models. To access it:

1. Click **Generative APIs** in the **AI** section of the side menu of the Scaleway console. The [list of available serverless models](https://console.scaleway.com/generative-apis/) displays.
2. Click <Icon name="more" /> next to the name of the model you want to use. Then click **Try model** in the pop-up menu. The **Playground chat** displays.
3. Enter your prompt in the form to start interacting with the model.

In the Playground, you can enter prompts, adjust model parameters, and immediately see the results of your queries. This is an excellent way to understand the models' capabilities and fine-tune your inputs for better results.

## Querying text models via API

While the Playground provides a hands-on environment for testing, you can also query the models programmatically using the OpenAI SDK.

### Installing the OpenAI SDK

First, ensure you have the OpenAI SDK installed in your development environment. You can install it using pip:

```bash
pip install openai
```

### Initializing the client

Initialize the OpenAI client with your base URL and API key:

```python
from openai import OpenAI

# Initialize the client with your base URL and API key
client = OpenAI(
    base_url="https://bccf05e2-55e1-4e42-aeec-87e153b7e552.ifr.fr-par.scw.cloud/v1",  # Scaleway's Generative AI endpoint
    api_key="<SCW_API_KEY>"  # Your unique API key from Scaleway
)
```

### Generating text with a chat completion

You can now create a chat completion, for example with the `llama-3-8b-instruct` model. Following you can find an example:

```python
# Create a chat completion using the 'llama-3-8b-instruct' model
completion = client.chat.completions.create(
    messages=[{"role": "user", "content": "Describe a futuristic city with advanced technology and green energy solutions."}],
    temperature=0.7,  # Adjusts creativity
    max_tokens=100,   # Limits the length of the output
    top_p=0.9         # Controls diversity through nucleus sampling
)

# Print the generated response
print(completion.choices[0].message["content"])
```

This code sends a message to the model and returns a text completion based on your input. The `temperature`, `max_tokens`, and `top_p` parameters control the response's creativity, length, and diversity, respectively.

### Model parameters and their effects

The following parameters can be adjusted to influence the output of the model:

- **`messages`**: A list of message objects that represent the conversation history. Each message should have a `role` (e.g., "user", "assistant") and `content`.
- **`temperature`**: Controls the output's randomness. Lower values (e.g., 0.2) make the output more deterministic, while higher values (e.g., 0.8) make it more creative.
- **`max_tokens`**: The maximum number of tokens (words or parts of words) in the generated output.
- **`top_p`**: An alternative to `temperature`, controlling the diversity of the output. It uses nucleus sampling, where the model considers the tokens with top probabilities until the cumulative probability reaches `top_p`.
- **`n`**: The number of completions to generate for the prompt. This is useful for getting multiple variations of the response.
- **`stop`**: A string or list of strings where the model will stop generating further tokens. This is useful for controlling the end of the output.

<Message type="tip">
 You can experiment with these parameters in the Playground to see how they affect the modelâ€™s responses.
</Message>

## Advanced usage tips

### Using the Playground for fine-tuning

The Playground allows you to test and tweak your queries and fine-tune parameters interactively. This can help you identify the best settings for your application without writing any code.

For example, you can:

- Adjust the `temperature` slider to see how it affects the creativity of the model.
- Modify the `max_tokens` to see how the length of the response changes.
- Set specific `stop` sequences to control where the model stops its output.

### Batch processing

For bulk query processing, you can use the API to send multiple prompts in a single request by using loops in your code. This can be particularly useful for generating large datasets or processing multiple inputs simultaneously.

Example:

```python
prompts = [
 {"role": "user", "content": "Tell me a story about a dragon."},
 {"role": "user", "content": "Describe the process of photosynthesis."},
 {"role": "user", "content": "What are the benefits of a healthy diet?"}
]

responses = [client.chat.completions.create(
    messages=[prompt],
    temperature=0.7,
    max_tokens=100,
    top_p=0.9
) for prompt in prompts]

for i, response in enumerate(responses):
    print(f"Response {i + 1}: {response.choices[0].message['content']}")
```

<Message type="note">
 If you encounter an error such as `Invalid API Key` or `Max tokens exceeded`, refer to the [Understanding errors documentation](/ai-data/generative-apis/api-cli/understanding-errors/) for troubleshooting tips.
</Message>