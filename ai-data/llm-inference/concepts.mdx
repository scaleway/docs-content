---
meta:
  title: LLM Endpoints - Concepts
  description: This page explains all the concepts related to Inference for LLMs
content:
  h1: LLM Endpoints - Concepts
  paragraph: This page explains all the concepts related to Inference for LLMs
tags: 
categories:
  - ia
---
## Allowed IPs

Access Control List (ACL) rules [define permissions for remote access to an Instance](/ai-data/llm-inference/how-to/manage-allowed-ip-addresses/). A rule consists of an IP address or an IP range. You can use them to define which host and networks can connect to your Database Instance's endpoint. You can add, edit, or delete rules from your ACLs. The initial setup of a Database Instance allows full network access from anywhere (`0.0.0.0/0`).

Access control is handled directly at the network level by Load Balancers, making the filtering more efficient and universal and relieving the Database Instance from this task.

## Context size

The context size refers to the length or size of the input text used to generate predictions or responses from a large language model. It is crucial in determining the model's understanding of the given prompt or query.

## Deployment

A deployment makes a trained language model available for real-world applications. It encompasses tasks such as integrating the model into existing systems, optimizing its performance, and ensuring scalability and reliability.

## Embedding Models

Embedding models are a representation-learning technique that converts textual data into numerical vectors. These vectors capture semantic information about the text and are often used as input to downstream machine-learning models or algorithms.

## Endpoint

An endpoint in the context of large language models refers to a network-accessible URL or interface through which clients can interact with the model for inference tasks. It exposes methods for sending input data and receiving model predictions or responses.

## Fine-tuning

Fine-tuning involves further training a pre-trained language model on domain-specific or task-specific data to improve performance on a particular task. This process often includes updating the model's parameters using a smaller, task-specific dataset.

## Few-shot prompting

Few-shot prompting uses the power of language models to generate responses with minimal input, relying on just a handful of examples or prompts.
It demonstrates the model's ability to generalize from limited training data to produce coherent and contextually relevant outputs.

## Inference

Inference is the process of deriving logical conclusions or predictions from available data. This concept involves using statistical methods, machine learning algorithms, and reasoning techniques to make decisions or draw insights based on observed patterns or evidence. 
Inference is fundamental in various AI applications, including natural language processing, image recognition, and autonomous systems.

## Large Language Model Applications

Large Language Model Applications are applications or software tools that leverage the capabilities of large language models for various tasks, such as text generation, summarization, or translation. These apps provide user-friendly interfaces for interacting with the models and accessing their functionalities.

## Large Language Models (LLMs)

Large Language Models (LLMs) are advanced artificial intelligence systems capable of understanding and generating human-like text on various topics.
These models, such as GPT-3, are trained on vast amounts of data to learn the patterns and structures of language, enabling them to generate coherent and contextually relevant responses to queries or prompts.
LLMs have applications in natural language processing, text generation, translation, and other tasks requiring sophisticated language understanding and production.

## Prompt

In the context of Large Language Models (LLMs), a prompt refers to the input provided to the model to generate a desired response.
It typically consists of a sentence, paragraph, or series of keywords or instructions that guide the model in producing text relevant to the given context or task.
The quality and specificity of the prompt greatly influence the generated output, as the model uses it to understand the user's intent and create responses accordingly.

## Quantization

Quantization is a technique used to reduce the precision of numerical values in a model's parameters or activations to improve efficiency and reduce memory footprint during inference. It involves representing floating-point values with fewer bits while minimizing the loss of accuracy.

## Retrieval Augmented Generation (RAG)

Retrieval Augmented Generation (RAG) is a framework combining information retrieval elements with language generation to enhance the capabilities of large language models. It involves retrieving relevant context or knowledge from external sources and incorporating it into the generation process to produce more informative and contextually grounded outputs.
