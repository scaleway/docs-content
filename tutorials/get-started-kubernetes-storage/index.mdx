---
meta:
  title: Getting started with Kubernetes Part 4 - Storage
  description: This is the fourth in a series of tutorials accompanying our video demonstrations on getting started with Kubernetes. Part 4 explains how to add storage to your cluster.
content:
  h1: Getting started with Kubernetes Part Part 4 - Storage
  paragraph: This is the fourth in a series of tutorials accompanying our video demonstrations on getting started with Kubernetes. Part 4 explains how to add storage to your cluster.
tags: Kubernetes k8s managed-kubernetes services kapsule kubectl storage volume block-storage stateful-set persistent-volume persistent-volume-claim 
categories:
  - kubernetes
dates:
  validation: 2024-01-29
  posted: 2024-01-29
---

This tutorial accompanies the fourth [video demonstration](TODOLINK) in our series to help users get started with Kubernetes. We walk you through Kubernetes fundamentals for beginners. In this installment, we show you how to add storage to your [Scaleway Kubernetes Kapsule](https://www.scaleway.com/en/kubernetes-kapsule/).  

First, we address the question of why we might need to add storage to our cluster, comparing **persistent** storage with **system** storage, and digging into **stateless** vs. **stateful** applications. We then show how to create a stateful application in a cluster via a **StatefulSet object**, which provisions persistent storage via a PersistentVolumeClaim, which in turns provisions and manages storage using the Scaleway Block Storage product.

Previous videos in this series covered the basics of [containers and Docker](/tutorials/get-started-containers-docker/), [deploying an app with Kapsule](/tutorials/get-started-deploy-kapsule/), and [adding a Load Balancer](/tutorials/get-started-kubernetes-loadbalancer/).

<Macro id="iam-requirements" />
 
<Message type="requirement">

  - You have an account and are logged into the [Scaleway console](https://console.scaleway.com)
  - You have [generated your API key](/identity-and-access-management/iam/how-to/create-api-keys/)
  - You have [created a Kubernetes Kapsule](/containers/kubernetes/how-to/create-cluster/) and [configured kubectl](/containers/kubernetes/how-to/connect-cluster-kubectl/).

</Message>

## Fundamental storage concepts

### System volume vs persistent volumes

It is important to differentiate between Kuberenetes **system volumes** and **persistent volumes**. This tutorial is concerned with **persistent volumes**, which can be provisioned and then used by applications inside the cluster to store data. Kuberenetes **system volumes** are created automatically but can only be used by the Kubernetes system itself to store essential files. This is summed up in the table below:

| System volume                            | Persistent volume                          |
| ---------------------------------------- | ------------------------------------------ |
| For the k8s system itself                |  For applications in the cluster           |
| Ephemeral (lives and dies with the node) | Persistent (beyond the life of a pod/node) |
| Auto-created                             | Must be provisioned                        |

You can view your cluster's system volumes in the Instances > Volumes section of the console, and see how they are linked to the Instances that make up the nodes of your cluster. However, you shouldn't attempt to use or manage these volumes - leave it to Kubernetes Kapsule. 

### Stateless applications vs stateful applications

What kind of applications need to use persistent volumes? To answer this, we must understand the difference between stateless and stateful applications.

Stateless applications do not need to load or save data: each request to the application is independent. There may be data in the payload of the request, but has not been retrieved from a previously saved state. A request to a search engine is a good example of this.

Stateful applications **do** need to save and load data. Most modern applications are stateless, such as online banking which needs to be able to store and retrieve all your transaction history, and online shopping which stores and remembers your previous orders. In order to run stateful applications in Kubernetes, we need to provision persistent storage volumes where they can store the data that gives them their state.

| Stateless applications                                      | Stateful applications                                                |
| ----------------------------------------------------------- | -------------------------------------------------------------------- |
| Do not load or save data                                    | Save and load data                                                   |
| Each request is independent and doesn't use previous data   | Need to retrieve stored data                                         |
| Examples: microservice, [whoami](/tutorials/get-started-containers-docker/#example-2-a-more-complex-app), search engine request     | Examples: most modern applications (online banking, online shopping) |
| "Original" Kubernetes                                       | Kubernetes has now adapted to incorporate statefulness               |
| Do not need to provision storage                            | Must provision storage                                               |

## How do we provision storage?

### Using the CSI to dynamically provision Block Storage volumes

Cloud providers such as Scaleway provide a **C**ontainer **S**torage **I**nterface (CSI) provisioner and default **StorageClasses** for their managed Kubernetes products. This defines the classes of storage the provider can offer for the cluster.

Scaleway provides [Block Storage](/storage/block/quickstart/) as its StorageClass, meaning that we offer persistent storage for Kapsule clusters via our Block Storage product. Thanks to dynamic volume provisioning, you can create storage volumes for your clusters on-demand: you simply describe the storage required and our CSI provisions it.

You can list the StorageClasses available to your cluster by using the command `kubectl get storageclass`.

<Message type="note">
Scaleway has upgraded the Block Storage used by our StorageClass, offering better performance. Our previous class was `scw-bss`, which has now been replaced by `sbs`, giving you faster read/write operations with lower latency.
</Message>

### Kubernetes objects: StatefulSets, PersistentVolumes and PersistentVolumeClaims

In order to provision persistent Block Storage resources in this tutorial, we will use a **PersistentVolumeClaim**. This is a request for, and claim to, a **PersistentVolume** resource. The PersistentVolumeClaim object request a specific size, access mode and StorageClass for the PersistentVolume, meaning we can describe what we want without needing to worry about how it's provisioned.

TODO: image

In our [previous tutorial](/tutorials/get-started-deploy-kapsule/) we used a **Deployment** object to deploy a stateless application. In this tutorial, we will use a **StatefulSet** object to deploy a stateful application.

Both Deloyments and StatefulSets manage the deployment and scaling of a set of pods with identical replications of a containerized application. But StatefulSets can also provision and manage persistent storage for thier pods, via PersistentVolumeClaims. What's more, in order to ensure data consistency and synchronicity as pods read and write to storage, the pods of a StatefulSet have sticky, persistent identities as shown below:

TODO: image





Kubernetes clusters 

<Lightbox src="scaleway-nodeport.webp" alt="" />

As mentioned, NodePort is a **Service**. It is important to differentiate between Services and Deployments in Kubernetes. Both oBy default, Kubernetes clusters are not exposed to the internet. This means that external users cannot access the application deployed in our cluster.

In the previous tutorial, we solved this problem by creating a **NodePort** Service for our cluster. NodePort is a Kubernetes service which opens a port on each node that contains pods for the specified deployment. It then forwards any external traffic received on that port to the right pods. This allowed us to go to our node’s external IP address in a browser (specifying the open port), and we were served our web application.f these are Kubernetes abstractions:

- **Deployments** keep a set of pods running with containerized applications
- **Services** enable network access to a specified set of pods.

Why then can’t we just keep using NodePort, why do we need a Load Balancer? 

NodePort is great for quick testing and can be adequate for single node, uncomplicated clusters. It is also free. However, it is not ideal for production, for reasons of security and ease of management. You are limited in which port numbers can be opened, and as your cluster starts to scale in complexity, maybe containing many microservices, NodePort gets less practical to use.

LoadBalancer is also a Kubernetes Service just like NodePort, and it is the standard service to use when you want to expose your cluster to the internet.

The LoadBalancer Service within your cluster creates an external Load Balancer. This external Load Balancer has a single IP address that forwards all traffic to the LoadBalancer Service within your cluster, which then takes care of forwarding it to the right pods.

<Lightbox src="scaleway-loadbalancer-service.webp" alt="" />

LoadBalancer supports multiple protocols and multiple ports per service, and is much more secure than NodePort. It provides all those things that are important and valued when it comes to Cloud Native technology: predictability, scalability and high availability.

## How do we create a Load Balancer?

First we create the LoadBalancer Service on our cluster by connecting to it via `kubectl` and creating a YAML manifest to specify the Service.

Then, the cluster’s Cloud Controller Manager (a component of the Kapsule’s control plane, managed by Scaleway) takes care of creating the external Scaleway Load Balancer, and is responsible for all of its configuration and management. We can check the console though, to see that the Load Balancer has indeed been created.

We use `kubectl` to check the IP address of our Load Balancer, and then we are ready to test!

<Message type="note">

It is not possible to create the external Scaleway Load Balancer yourself via the console/API and then connect it to your cluster afterward. You must create the LoadBalancer Service in the cluster first, so that the Scaleway Cloud Controller Manager in your Kapsule’s control plane can handle the creation of the Scaleway Load Balancer, with the right configuration to direct its traffic to the correct pods of your cluster.

</Message>

## Step 1: Create Load Balancer from YAML manifest

1. Open a terminal, and run the following command to check the state of your cluster:
    ```
    kubectl get all
    ```
    You should see two pods, the NodePort Service created during the previous tutorial (along with the basic default ClusterIP Service), the deployment and the two ReplicaSets.
2. Delete the NodePort Service with the following command:
    ```
    kubectl delete svc name-of-nodeport-service
    ```
    In our case, we called our NodePort service `mydeployment`, so the command is `kubectl delete svc mydeployment`.
3. Create a new file called `lb.yaml`:
    ```
    nano lb.yaml
    ```
    This file will be a manifest for our LoadBalancer Service.
4. Copy and paste the following text into the file:
    ```
    apiVersion: v1
    kind: Service
    metadata:
      name: myloadbalancer
      labels:
        app: mydeployment
    spec:
      type: LoadBalancer
      ports:
      - port: 8000
        name: http
        targetPort: 8000
      selector:
        app: mydeployment
    ```
    - **apiVersion** specifies which version of the Kubernetes API to use to create the object
    - **kind** specifies the kind of object defined in this YAML file, here a Service
    - **metadata** helps uniquely identify our Service object: we give it a name (`myloadbalancer`), and a label.
    - **spec** specifies the Service. It is of LoadBalancer **type**. We then go on to specify its **ports**. We can define many ports if we want but here we just specify the necessary port `8000` for our `whoami` app. Since `8000` is an HTTP port, we add a name tag and call it `http`. **targetPort** is the port the container welcomes traffic (in our case necessarily `8000`), **port** is the abstracted Service port. For simplicity, we set both as `8000`, though we could change **port** to something else.
    - **selector** tells the Service which pods to redirect to: in this case pods with containers running the app we called `mydeployment`
  Save and exit the file.
5. Tell Kubernetes to create the Load Balancer from the manifest we just created with the following command:
    ```
    kubectl create -f lb.yaml
    ```

  A message displays to confirm the creation of the Load Balancer.

## Step 2: Check Load Balancer in the console

1. Open a browser and go to [console.scaleway.com](http://console.scaleway.com).
2. Select **Network** > **Load Balancers** from the side menu.
  A list of your Scaleway Load Balancers displays. You should see one with the tags `kapsule` and `cluster=xxxx`. This is the Load Balancer created by the Cloud Controller Manager of your Kubernetes Kapsule cluster when you created your LoadBalancer Service in the previous step.
3. Click the LoadBalancer to view its details.
  <Lightbox src="scaleway-load-balancer.webp" alt="" />
  The Load Balancer is auto-managed by Kapsule, so no changes can be made here directly.
  In the `IPv4` field, the Load Balancer's IP address displays.

## Step 3: Check Load Balancer IP via kubectl

Let's check that the IP address seen in the console is also known to our cluster:

1. Run the following command:
  ```
  kubectl get svc
  ```
  An output similar to the following displays:
  ```
  NAME             TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)          AGE
  kubernetes       ClusterIP      10.32.0.1       <none>           443/TCP          13d
  myloadbalancer   LoadBalancer   10.43.204.154   51.159.207.167   8000:31725/TCP   27h
  ```
  We see that the Load Balancer's external IP is the same as the one we saw in the console in the previous step.

## Step 4: Test

  In a browser, enter the external IP address of your Load Balancer, followed by the port 8000 that we opened, e.g. `158.58.37:8000`.

  A text similar to the following should display, showing that the containerized `whoami` application is running on our cluster and accessible through our Load Balancer:
  ```
  I'm mydeployment-6579f975d55-fv4sx
  ```

<Navigation title="See also">
  <PreviousButton to="/tutorials/get-started-deploy-kapsule/">Getting started with Kubernetes Part 2 - Deploying an app with Kapsule</PreviousButton>
</Navigation>