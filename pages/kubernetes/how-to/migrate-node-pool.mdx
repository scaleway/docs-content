---
meta:
  title: How to migrate Kubetnetes workloads to a new node pool
  description: Learn how to migrate existing Kubernetes workloads to a new node pool.
content:
  h1: How to migrate Kubetnetes workloads to a new node pools
  paragraph: Learn how to migrate existing Kubernetes workloads to a new node pool.
tags: kubernetes kapsule kosmos
dates:
  validation: 2025-06-23
  posted: 2025-06-23
categories:
  - containers
---

This documentation provides step-by-step instructions on how to migrate Kubernetes workloads from one node pool to another within a Kubernetes Kapsule cluster.
Migrating workloads can be required to change the commercial type of Instance for your pool, or to scale your infrastructure.

<Macro id="requirements" />

- A Scaleway account logged into the [console](https://console.scaleway.com)
- [Owner](/iam/concepts/#owner) status or [IAM permissions](/iam/concepts/#permission) allowing you to perform actions in the intended Organization
- Created a [Kubernetes Kapsule cluster](/kubernetes/how-to/create-cluster/)
- Have an existing node pool that you want to migrate

<Message type="important">
  Always ensure that your **data is backed up** before performing any operations that could affect it.
</Message>

1. Create the new node pool with the desired configuration either [from the console](/kubernetes/how-to/create-node-pool/) or by using `kubectl`.
    <Message type="tip">
      Ensure that the new node pool is properly labeled if necessary.
    </Message>

2. Run `kubectl get nodes` to check that the new nodes are in a `Ready` state.

3. Cordon the nodes in the old node pool to prevent new pods from being scheduled there. For each node, run: `kubectl cordon <node-name>`

4. Drain the nodes to evict the pods gracefully.
   - For each node, run: `kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data`
   - The `--ignore-daemonsets` flag is used because daemon sets manage pods across all nodes and will automatically reschedule them.
   - The `--delete-emptydir-data` flag is necessary if your pods use emptyDir volumes, but use this option carefully as it will delete the data stored in these volumes.
   - Refer to the [official Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/) for further information.

5. Run `kubectl get pods -o wide` after draining, to verify that the pods have been rescheduled to the new node pool.

6. [Delete the old node pool](/kubernetes/how-to/delete-node-pool/) once you confirm that all workloads are running smoothly on the new node pool.
