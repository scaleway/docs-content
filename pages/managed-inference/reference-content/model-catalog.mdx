---
title: Managed Inference model catalog
description: Deploy your own model with Scaleway Managed Inference. Privacy-focused, fully managed.
tags:
dates:
  validation: 2025-04-18
  posted: 2024-04-18
---
A quick overview of available models in Scaleway's catalog and their core attributes. Expand any model below to see usage examples, curl commands, and detailed capabilities.

<Message type="tip">
  For more information about all the models supported in Managed Inference, refer to the [Supported Models in Managed Inference](/managed-inference/reference-content/supported-models/) page.
</Message>

## Models technical summary

| Model name | Provider | Maximum Context length (tokens) | Modalities | Compatible Instances (Max Context in tokens\*) | License |
|------------|----------|--------------|------------|-----------|---------|
| [`gpt-oss-120b`](#gpt-oss-120b) | OpenAI | 128k | Text | H100 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`whisper-large-v3`](#whisper-large-v3) | OpenAI | - | Audio transcription | L4, L40S, H100, H100-SXM-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`qwen3-235b-a22b-instruct-2507`](#qwen3-235b-a22b-instruct-2507) | Qwen | 40k | Text | H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`gemma-3-27b-it`](#gemma-3-27b-it) | Google | 40k | Text, Vision | H100, H100-2 | [Gemma](https://ai.google.dev/gemma/terms) |
| [`llama-3.3-70b-instruct`](#llama-33-70b-instruct) | Meta | 128k | Text | H100 (15k), H100-2 | [Llama 3.3 Community](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) |
| [`llama-3.1-70b-instruct`](#llama-31-70b-instruct) | Meta | 128k | Text | H100 (15k), H100-2 | [Llama 3.1 Community](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct/blob/main/LICENSE) |
| [`llama-3.1-8b-instruct`](#llama-31-8b-instruct) | Meta | 128k | Text | L4 (90k), L40S, H100, H100-2 | [Llama 3.1 Community](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/blob/main/LICENSE) |
| [`llama-3-70b-instruct`](#llama-3-70b-instruct) | Meta | 8k | Text | H100, H100-2 | [Llama 3 Community](https://huggingface.co/meta-llama/Meta-Llama-3-8B/blob/main/LICENSE) |
| [`llama-3.1-nemotron-70b-instruct`](#llama-31-nemotron-70b-instruct) | Nvidia | 128k | Text | H100 (15k), H100-2 | [Llama 3.1 Community](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct/blob/main/LICENSE) |
| [`deepseek-r1-distill-70b`](#deepseek-r1-distill-llama-70b) | Deepseek | 128k | Text | H100 (13k), H100-2 | [MIT](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B/blob/main/LICENSE) and [Llama 3.3 Community](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE) |
| [`deepseek-r1-distill-8b`](#deepseek-r1-distill-llama-8b) | Deepseek | 128k | Text | L4 (90k), L40S, H100, H100-2 | [MIT](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B/blob/main/LICENSE) and [Llama 3.1 Community](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/blob/main/LICENSE) |
| [`mistral-7b-instruct-v0.3`](#mistral-7b-instruct-v03) | Mistral | 32k | Text | L4, L40S, H100, H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`mistral-small-3.2-24b-instruct-2506`](#mistral-small-32-24b-instruct-2506) | Mistral | 128k | Text, Vision | H100, H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`mistral-small-3.1-24b-instruct-2503`](#mistral-small-31-24b-instruct-2503) | Mistral | 128k | Text, Vision | H100, H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`mistral-small-24b-instruct-2501`](#mistral-small-24b-instruct-2501) | Mistral | 32k | Text | L40S (20k), H100, H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`voxtral-small-24b-2507`](#voxtral-small-24b-2507) | Mistral | 32k | Text | H100, H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`mistral-nemo-instruct-2407`](#mistral-nemo-instruct-2407) | Mistral | 128k | Text | L40S, H100, H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`mixtral-8x7b-instruct-v0.1`](#mixtral-8x7b-instruct-v01) | Mistral | 32k | Text | H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`magistral-small-2506`](#magistral-small-2506) | Mistral | 32k | Text | L40S, H100, H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`devstral-small-2505`](#devstral-small-2505) | Mistral | 128k | Text | H100, H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`pixtral-12b-2409`](#pixtral-12b-2409) | Mistral | 128k | Text, Vision | L40S (50k), H100, H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`molmo-72b-0924`](#molmo-72b-0924) | Allen AI | 50k | Text, Vision | H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) and [Twonyi Qianwen license](https://huggingface.co/Qwen/Qwen2-72B/blob/main/LICENSE)|
| [`qwen3-coder-30b-a3b-instruct`](#qwen3-coder-30b-a3b-instruct) | Qwen | 128k | Code | L40S, H100, H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`qwen2.5-coder-32b-instruct`](#qwen25-coder-32b-instruct) | Qwen | 32k | Code | H100, H100-2 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [`bge-multilingual-gemma2`](#bge-multilingual-gemma2) |  BAAI | 4k | Embeddings | L4, L40S, H100, H100-2 | [Gemma](https://ai.google.dev/gemma/terms) |
| [`sentence-t5-xxl`](#sentence-t5-xxl) | Sentence transformers | 512 | Embeddings | L4 | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) |

\*Maximum context length is only mentioned when instances VRAM size limits context length. Otherwise, maximum context length is the one defined by the model. 

## Models feature summary
| Model name | Structured output supported | Function calling | Supported languages |
| --- | --- | --- | --- |
| `gpt-oss-120b` | Yes | Yes | English |
| `whisper-large-v3` | - | - | English, French, German, Chinese, Japanese, Korean and 81 additional languages  |
| `qwen3-235b-a22b-instruct-2507` | Yes | Yes | English, French, German, Chinese, Japanese, Korean and 113 additional languages and dialects |
| `gemma-3-27b-it` | Yes | Partial | English, Chinese, Japanese, Korean and 31 additional languages |
| `llama-3.3-70b-instruct` | Yes | Yes | English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai |
| `llama-3.1-70b-instruct` | Yes | Yes | English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai |
| `llama-3.1-8b-instruct` | Yes | Yes | English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai |
| `llama-3-70b-instruct` | Yes | No | English |
| `llama-3.1-nemotron-70b-instruct` | Yes | Yes | English |
| `deepseek-r1-distill-llama-70B` | Yes | Yes | English, Chinese |
| `deepseek-r1-distill-llama-8B` | Yes | Yes | English, Chinese |
| `mistral-7b-instruct-v0.3` | Yes | Yes | English |
| `mistral-small-3.2-24b-instruct-2506` | Yes | Yes | English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, Farsi |
| `mistral-small-3.1-24b-instruct-2503` | Yes | Yes | English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, Farsi |
| `mistral-small-24b-instruct-2501` | Yes | Yes | English, French, German, Dutch, Spanish, Italian, Polish, Portuguese, Chinese, Japanese, Korean |
| `voxtral-small-24b-2507` | Yes | Yes | English, French, German, Dutch, Spanish, Italian, Portuguese, Hindi |
| `mistral-nemo-instruct-2407` | Yes | Yes | English, French, German, Spanish, Italian, Portuguese, Russian, Chinese, Japanese |
| `mixtral-8x7b-instruct-v0.1` | Yes | No | English, French, German, Italian, Spanish |
| `magistral-small-2506` | Yes | Yes | English, French, German, Spanish, Portuguese, Italian, Japanese, Korean, Russian, Chinese, Arabic, Persian, Indonesian, Malay, Nepali, Polish, Romanian, Serbian, Swedish, Turkish, Ukrainian, Vietnamese, Hindi, Bengali |
| `devstral-small-2505` | Yes | Yes | English, French, German, Spanish, Portuguese, Italian, Japanese, Korean, Russian, Chinese, Arabic, Persian, Indonesian, Malay, Nepali, Polish, Romanian, Serbian, Swedish, Turkish, Ukrainian, Vietnamese, Hindi, Bengali |
| `pixtral-12b-2409` | Yes | Yes | English |
| `molmo-72b-0924` | Yes | No | English |
| `qwen3-coder-30b-a3b-instruct` | Yes | Yes | English, French, German, Chinese, Japanese, Korean and 113 additional languages and dialects |
| `qwen2.5-coder-32b-instruct` | Yes | Yes | English, French, Spanish, Portuguese, German, Italian, Russian, Chinese, Japanese, Korean, Vietnamese, Thai, Arabic and 16 additional languages. |
| `bge-multilingual-gemma2` | No | No | English, French, Chinese, Japanese, Korean |
| `sentence-t5-xxl` | No | No | English |


## Model details
<Message type="note">
  Despite efforts for accuracy, the possibility of generated text containing inaccuracies or [hallucinations](/managed-inference/concepts/#hallucinations) exists. Always verify the content generated independently.
</Message>

## Multimodal models (Text and Vision)

<Message type="note">
  Vision models can understand and analyze images, not generate them. You will use it through the /v1/chat/completions endpoint.
</Message>

### Gemma-3-27b-it
Gemma-3-27b-it is a model developed by Google to perform text processing and image analysis on many languages.
The model was not trained specifically to output function / tool call tokens. Hence function calling is currently supported, but reliability remains limited.

#### Model names
```
google/gemma-3-27b-it:bf16
```
| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | No |
| Supported image formats | PNG, JPEG, WEBP, and non-animated GIFs |
| Maximum image resolution (pixels) | 896x896 |
| Token dimension (pixels)| 56x56 |

- Pan & Scan is not yet supported for Gemma 3 images. This means that high resolution images are currently resized to 896x896 resolution that may generate artifacts and lead to a lower accuracy.

### Mistral-small-3.2-24b-instruct-2506
Mistral-small-3.2-24b-instruct-2506 is an improved version of Mistral-small-3.1 which performs better on tool calling.
This model was optimized to have a dense knowledge and faster tokens throughput compared to its size.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | Yes |
| Supported images formats | PNG, JPEG, WEBP, and non-animated GIFs |
| Maximum image resolution (pixels) | 1540x1540 |
| Token dimension (pixels)| 28x28 |

#### Model names
```
mistral/mistral-small-3.2-24b-instruct-2506:fp8
```

### Mistral-small-3.1-24b-instruct-2503
Mistral-small-3.1-24b-instruct-2503 is a model developed by Mistral to perform text processing and image analysis on many languages.
This model was optimized to have a dense knowledge and faster tokens throughput compared to its size.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | Yes |
| Supported images formats | PNG, JPEG, WEBP, and non-animated GIFs |
| Maximum image resolution (pixels) | 1540x1540 |
| Token dimension (pixels)| 28x28 |

#### Model names
```
mistral/mistral-small-3.1-24b-instruct-2503:bf16
mistral/mistral-small-3.1-24b-instruct-2503:fp8
```

- Bitmap (or raster) image formats, meaning storing images as grids of individual pixels, are supported. Vector image formats (SVG, PSD) are not supported, neither PDFs nor videos.
- Images size are limited in the following ways:
  - Directly by the maximum context window. As an example, since tokens are squares of 28x28 pixels, the maximum context window taken by a single image is `3025` tokens (ie. `(1540*1540)/(28*28)`)
  - Indirectly by the model accuracy: resolution above 1540x1540 will not increase model output accuracy. Indeed, images above 1540 pixels width or height will be automatically downscaled to fit within 1540x1540 dimension. Note that image ratio and overall aspect is preserved (images are not cropped, only additionaly compressed).

### Pixtral-12b-2409
Pixtral is a vision language model introducing a novel architecture: 12B parameter multimodal decoder plus 400M parameter vision encoder.
It can analyze images and offer insights from visual content alongside text.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | Yes |
| Supported images formats | PNG, JPEG, WEBP, and non-animated GIFs |
| Maximum image resolution (pixels) | 1024x1024 |
| Token dimension (pixels)| 16x16 |
| Maximum images per request | 12 |

#### Model name
```
mistral/pixtral-12b-2409:bf16
```

### Molmo-72b-0924
Molmo 72B is the powerhouse of the Molmo family, multimodal models developed by the renowned research lab Allen Institute for AI.
Vision-language models like Molmo can analyze an image and offer insights from visual content alongside text. This multimodal functionality creates new opportunities for applications that need both visual and textual comprehension.

#### Model name
```
allenai/molmo-72b-0924:fp8
```

## Multimodal models (Text and Audio)

### Voxtral-small-24b-2507
Voxtral-small-24b-2507 is a model developed by Mistral to perform text processing and audio analysis on many languages.
This model was optimized to enable transcription in many languages while keeping conversational capabilities (translations, classification, etc.)

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | Yes |
| Supported audio formats | WAV and MP3 |
| Audio chunk duration | 30 seconds |
| Token duration (audio)| 80ms |
| Maximum transcription duration| 30 minutes |
| Maximum understanding duration| 40 minutes |

#### Model names
```
mistral/voxtral-small-24b-2507:bf16
mistral/voxtral-small-24b-2507:fp8
```

- Mono and stereo audio formats are supported. For stereo formats, both left and right channels are merged before being processed.
- Audio files are processed in 30 seconds chunks:
  - If audio sent is less than 30 seconds, the rest of the chunk will be considered silent. 
  - 80ms is equal to 1 input token

## Audio transcription models

### Whisper-large-v3
Whisper-large-v3 is a model developed by OpenAI to transcribe audio in many languages.
This model is optimized for audio transcription tasks.

| Attribute | Value |
|-----------|-------|
| Supported audio formats | WAV and MP3 |
| Audio chunk duration | 30 seconds |

#### Model names
```
openai/whisper-large-v3:bf16
```

- Mono and stereo audio formats are supported. For stereo formats, both left and right channels are merged before being processed.
- Audio files are processed in 30 seconds chunks:
  - If audio sent is less than 30 seconds, the rest of the chunk will be considered silent. 

## Text models

### Qwen3-235b-a22b-instruct-2507
Released July 23, 2025, Qwen 3 235B A22B is an open-weight model, competitive in multiple benchmarks (such as [LM Arena for text use cases](https://lmarena.ai/leaderboard)) compared to Gemini 2.5 Pro and GPT4.5.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | Yes |



#### Model name
```
openai/gpt-oss-120b:fp4
```

### Gpt-oss-120b
Released August 5, 2025, GPT OSS 120B is an open-weight model providing significant throughput performance and reasoning capabilities.
Currently, this model should be used through Responses API, as Chat Completion does not yet support tool calling for this model.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | Yes |

#### Model name
```
openai/gpt-oss-120b:fp4
```

### Llama-3.3-70b-instruct
Released December 6, 2024, Meta’s Llama 3.3 70b is a fine-tune of the [Llama 3.1 70b](/managed-inference/reference-content/model-catalog/#llama-31-70b-instruct) model.
This model is still text-only (text in/text out). However, Llama 3.3 was designed to approach the performance of Llama 3.1 405B on some applications.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | Yes |

#### Model name
```
meta/llama-3.3-70b-instruct:fp8
meta/llama-3.3-70b-instruct:bf16
```

### Llama-3.1-70b-instruct
Released July 23, 2024, Meta’s Llama 3.1 is an iteration of the open-access Llama family.
Llama 3.1 was designed to match the best proprietary models and outperform many of the available open source on common industry benchmarks.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | Yes |

#### Model names
```
meta/llama-3.1-70b-instruct:fp8
meta/llama-3.1-70b-instruct:bf16
```

### Llama-3.1-8b-instruct
Released July 23, 2024, Meta’s Llama 3.1 is an iteration of the open-access Llama family.
Llama 3.1 was designed to match the best proprietary models and outperform many of the available open source on common industry benchmarks.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | Yes |

#### Model names
```
meta/llama-3.1-8b-instruct:fp8
meta/llama-3.1-8b-instruct:bf16
```

### Llama-3-70b-instruct
Meta’s Llama 3 is an iteration of the open-access Llama family.
Llama 3 was designed to match the best proprietary models, enhanced by community feedback for greater utility and responsibly spearheading the deployment of LLMs.
With a commitment to open-source principles, this release marks the beginning of a multilingual, multimodal future for Llama 3, pushing the boundaries in reasoning and coding capabilities.

#### Model name
```
meta/llama-3-70b-instruct:fp8
```

### Llama-3.1-Nemotron-70b-instruct
Introduced October 14, 2024, NVIDIA's Nemotron 70B Instruct is a specialized version of the Llama 3.1 model designed to follow complex instructions.
NVIDIA employed Reinforcement Learning from Human Feedback (RLHF) to fine-tune the model’s ability to generate relevant and informative responses.

#### Model name
```
nvidia/llama-3.1-nemotron-70b-instruct:fp8
```

### DeepSeek-R1-Distill-Llama-70B
Released January 21, 2025, Deepseek’s R1 Distilled Llama 70B is a distilled version of the Llama model family based on Deepseek R1.
DeepSeek R1 Distill Llama 70B is designed to improve the performance of Llama models on reasoning use cases such as mathematics and coding tasks.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | No |

#### Model name
```
deepseek/deepseek-r1-distill-llama-70b:fp8
deepseek/deepseek-r1-distill-llama-70b:bf16
```

### DeepSeek-R1-Distill-Llama-8B
Released January 21, 2025, Deepseek’s R1 Distilled Llama 8B is a distilled version of the Llama model family based on Deepseek R1.
DeepSeek R1 Distill Llama 8B is designed to improve the performance of Llama models on reasoning use cases such as mathematics and coding tasks.

#### Model names
```
deepseek/deepseek-r1-distill-llama-8b:fp8
deepseek/deepseek-r1-distill-llama-8b:bf16
```

### Mixtral-8x7b-instruct-v0.1
Mixtral-8x7b-instruct-v0.1, developed by Mistral, is tailored for instructional platforms and virtual assistants.
Trained on vast instructional datasets, it provides clear and concise instructions across various domains, enhancing user learning experiences.

#### Model names
```
mistral/mixtral-8x7b-instruct-v0.1:fp8
mistral/mixtral-8x7b-instruct-v0.1:bf16
```

### Mistral-7b-instruct-v0.3
The first dense model released by Mistral AI, perfect for experimentation, customization, and quick iteration. At the time of the release, it matched the capabilities of models up to 30B parameters.
This model is open-weight and distributed under the Apache 2.0 license.

#### Model name
```
mistral/mistral-7b-instruct-v0.3:bf16
```

### Mistral-small-24b-instruct-2501
Mistral Small 24B Instruct is a state-of-the-art transformer model of 24B parameters, built by Mistral.
This model is open-weight and distributed under the Apache 2.0 license.

#### Model name
```
mistral/mistral-small-24b-instruct-2501:fp8
mistral/mistral-small-24b-instruct-2501:bf16
```

### Mistral-nemo-instruct-2407
Mistral Nemo is a state-of-the-art transformer model of 12B parameters, built by Mistral in collaboration with NVIDIA.
This model is open-weight and distributed under the Apache 2.0 license.
It was trained on a large proportion of multilingual and code data.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | Yes |

#### Model name
```
mistral/mistral-nemo-instruct-2407:fp8
```

### Magistral-small-2506
Magistral Small is a reasoning model, optimized to perform well on reasoning tasks such as academic or scientific questions.
It is well suited for complex tasks requiring multiple reasoning steps.

#### Model name
```
mistral/magistral-small-2506:fp8
mistral/magistral-small-2506:bf16
```

### Devstral-small-2505
Devstral Small is a fine-tune of Mistral Small 3.1, optimized to perform software engineering tasks.
It is a good fit to be used as coding agent, for instance in an IDE.

#### Model name
```
mistral/devstral-small-2505:fp8
mistral/devstral-small-2505:bf16
```

## Code models

### Qwen3-coder-30b-a3b-instruct
Qwen3-coder is an improved version of Qwen2.5 with better accuracy and throughput. 
Thanks to its a3b architecture, only a subset of its weights are activated for a given generation, leading to much faster input and output token processing, ideal for code completion.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | Yes |

#### Model name
```
qwen/qwen3-coder-30b-a3b-instruct:fp8
```

### Qwen2.5-coder-32b-instruct
Qwen2.5-coder is your intelligent programming assistant familiar with more than 40 programming languages.
With Qwen2.5-coder deployed at Scaleway, your company can benefit from code generation, AI-assisted code repair, and code reasoning.

| Attribute | Value |
|-----------|-------|
| Supports parallel tool calling | No |

#### Model name
```
qwen/qwen2.5-coder-32b-instruct:int8
```

## Embeddings models

### Bge-multilingual-gemma2
BGE-Multilingual-Gemma2 tops the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard), scoring the number one spot in French and Polish, and number seven in English (as of Q4 2024).
As its name suggests, the model’s training data spans a broad range of languages, including English, Chinese, Polish, French, and more.

| Attribute | Value |
|-----------|-------|
| Embedding dimensions | 3584 |
| Matryoshka embedding | No |

<Message type="note">
  [Matryoshka embeddings](https://huggingface.co/blog/matryoshka) refers to embeddings trained on multiple dimensions number. As a result, resulting vectors dimensions will be sorted by most meaningful first. For example, a 3584 dimensions vector can be truncated to its 768 first dimensions and used directly.
</Message>

#### Model name
```
baai/bge-multilingual-gemma2:fp32
```

### Sentence-t5-xxl
The Sentence-T5-XXL model represents a significant evolution in sentence embeddings, building on the robust foundation of the Text-To-Text Transfer Transformer (T5) architecture.
Designed for performance in various language processing tasks, Sentence-T5-XXL leverages the strengths of T5's encoder-decoder structure to generate high-dimensional vectors that encapsulate rich semantic information.
This model has been meticulously tuned for tasks such as text classification, semantic similarity, and clustering, making it a useful tool in the RAG (Retrieval-Augmented Generation) framework. It excels in sentence similarity tasks, but its performance in semantic search tasks is less optimal.


| Attribute | Value |
|-----------|-------|
| Embedding dimensions | 768 |
| Matryoshka embedding | No |

#### Model name
```
sentence-transformers/sentence-t5-xxl:fp32
```
