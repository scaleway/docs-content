---
meta:
  title: How to use the preinstalled environment
  description: This page explains how to use the preinstalled environment on a Scaleway Docker image for a GPU Instance
content:
  h1: How to use the preinstalled environment
  paragraph: This page explains how to use the preinstalled environment
dates:
  validation: 2022-03-25
  posted: 2022-03-25
---


GPU Instances have different types of pre-installed environment, depending on the OS image you chose during [creation of the Instance](/compute/gpu/how-to/create-manage-gpu-instance):

|        OS image       | Image release type |    Preinstalled on image                                        | Working environment                            |
|:---------------------:|:------------------:|:---------------------------------------------------------------:|------------------------------------------------|
| Ubuntu Focal GPU OS11 | Latest             | Nvidia drivers, Nvidia Docker environment (launch Docker container to access working environment) | pipenv virtual environment accessed via Docker |
| Ubuntu Bionic ML 10.1 | Legacy             | Nvidia drivers, CUDA 10.1, Nvidia Docker environment, ready-to-use "ai" conda environment      | conda environment                              |
| Ubuntu Bionic ML 9.2  | Legacy             | Nvidia drivers, CUDA 9.1, Nvidia Docker environment, ready-to-use "ai" conda environment       | conda environment                              |

Using the latest Ubuntu Focal GPU OS11 image gives you a minimal OS installation on which you can [launch](/compute/gpu/how-to/use-gpu-with-docker) one of our ready-made [Docker images](/compute/gpu/reference-content/docker-images). This gives you access to a pre-installed Python environment managed with pipenv. A number of useful AI core packages and tools are installed, including scipy, numpy, scikit-learn, jupyter, tensorflow and the Scaleway SDK. Depending on the [Docker image you choose](/compute/gpu/reference-content/docker-images), other packages and tools will also be pre-installed, providing a convenient framework environment for you so that you can begin work immediately.

Using one of the legacy Ubuntu Bionic ML images gives you direct access to a ready-to-use Python environment pre-installed on the OS, managed with conda. As Docker is pre-installed, you can also choose to launch a Docker image to access a different working environment.

<Message type="requirement">

- You have an account and are logged into the [Scaleway Console](https://console.scaleway.com) 
- You have created a [GPU Instance](/compute/gpu/how-to/create-manage-gpu-instance)
- You have [generated your SSH key](/console/my-project/how-to/create-ssh-key/)

</Message>

## Working with the pre-installed environment on Ubuntu Bionic ML legacy images

1. [Connect to your Instance via SSH](/compute/instances/how-to/connect-to-instance/).

  You are now directly within the conda `ai` pre-installed environment, and can get right to work.

2. Use the [official conda documentation](https://docs.conda.io/projects/conda/en/latest/commands.html) if you need any help managing your conda environment.

  <Message type="tip">

  For a full, detailed list of the Python packages and versions pre-installed in this environment, look at the content of the `/root/conda-ai-env-requirements.frozen` file.

  </Message>

  As Docker is also pre-installed, you could choose to [launch one of Scaleway's ready-made Docker images](/compute/gpu/how-to/use-gpu-with-docker) to access our latest working environments, if you wish.

3. Type `exit` to disconnect from your GPU Instance when you have finished your work.

## Working with the pre-installed environment on Ubuntu Focal GPU OS11

1. [Connect to your Instance via SSH](/compute/instances/how-to/connect-to-instance/).

  You are now connected to your Instance, and see your OS. A minimum of packages, including Docker, are installed. Pipenv is **not** pre-installed here. You must launch a Scaleway AI Docker container to access the preinstalled Pipenv environment.

2. [Launch](/compute/gpu/how-to/use-gpu-with-docker) one of our ready-made [Docker images](/compute/gpu/reference-content/docker-images).

  You are now in the `ai` directoy of the Docker container, in the activated pipenv virtual environment, and can get right to work!

  <Message type="tip">

  Use the command `pipenv graph` to see a list of all installed packages and their versions, as well as all the dependencies of each package. For more help with pipenv, see our [dedicated documentation](/compute/gpu/how-to/use-pipenv).

  </Message>

### Launching an application in your local browser

Some applications, such as [Jupyter Lab](https://jupyter.org/) or [Code Server](https://code-server.dev/), require a browser to run. You can launch these from the `ai` virtual environment of your Docker container, and view them in the browser of your local machine. This is thanks to the port mapping we specified when we ran the Docker container (the `-p 8888:8888 -p 6006:6006` arguments of the `docker run` command).

1. Launch an application. Here, we launch Jupyter Lab:

  `jupyter-lab`

  Within the output, you should see something similar to the following:

  ```
  [I 2022-04-06 11:38:40.554 ServerApp] Serving notebooks from local directory: /home/jovyan/ai
  [I 2022-04-06 11:38:40.554 ServerApp] Jupyter Server 1.15.6 is running at:
  [I 2022-04-06 11:38:40.554 ServerApp] http://7d783f7cf615:8888/lab?token=e0c21db2665ac58c3cf124abf43927a9d27a811449cb356b
  [I 2022-04-06 11:38:40.555 ServerApp]  or http://127.0.0.1:8888/lab?token=e0c21db2665ac58c3cf124abf43927a9d27a811449cb356b
  [I 2022-04-06 11:38:40.555 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
  ```

2. On your local computer, open a browser window and enter the following URL. Replace `<ip-address>` with the IP address of your Scaleway GPU Instance, and `<my-token>` with the token shown displayed in the last lines of terminal output after theh `jupyter-lab` command

  ```
  <ip-address>:8888/lab?token=<my-token>
  ```

  <Message type="tip">

  You can find the IP address of your Instance in the Scaleway console. In the side menu, click **Instances** to see a list of your Instances. The IP address of each of them is shown in the list that displays.

  </Message>

  Jupyter Lab now displays in your browser. You can use the Notebook, Console, or other features as required:

  <Lightbox src="scaleway-jupyter-lab.png" alt="" />

3. Use CTRL+C in the terminal window of your GPU Instance / Docker container to close down the Jupyter server when you've finished.

<Message type="note">

To launch Visual Studio Code, you can access the required password with the command `cat ~/.config/code-server/config.yaml | grep password`

</Message>


### Exiting the pre-installed environment and the container

When you are in the activated pipenv virtual environment, your command line prompt will normally be prefixed by the name of the environment. Here, for example, from `(ai) jovyan@d73f1fa6bf4d:~/ai` we see that we are in the activated `ai` environment, and from `jovyan@d73f1fa6bf4d:~/ai` that we are in the `\ai` directory of our container:

1. Type `exit` the following command to leave the pre-installed `ai` environment.

  You are now outside the pre-installed virtual environment.

2. Type `exit` again to exit the Docker container.

  Your prompt should now look similar to the following. You are still connected to your GPU Instance, but you have left the Docker container:

  ```
  root@scw-name-of-instance:~#
  ```

3. Type `exit` once more to disconnect from your GPU Instance.

<Navigation title="See Also">
  <PreviousButton to="/compute/gpu/how-to/use-gpu-with-docker/">How to access the GPU using Docker</PreviousButton>
  <NextButton to="/compute/gpu/how-to/use-pipenv">How to use pipenv to create virtual environments</NextButton>
</Navigation>
