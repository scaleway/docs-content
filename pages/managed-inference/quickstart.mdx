---
title: Managed Inference - Quickstart
description: Start with Scaleway Managed Inference for secure, scalable AI model deployment in Europe's premier platform. Privacy-focused, fully managed.
tags:
dates:
  validation: 2025-07-21
categories:
  - ai-data
---
import Requirements from '@macros/iam/requirements.mdx'


Scaleway Managed Inference is the first European Managed Inference platform on the market. It is a scalable and secure inference engine for Large Language Models (LLMs).

Scaleway Managed Inference is a fully managed service that allows you to serve generative AI models in a production environment.
With Scaleway Managed Inference, you can easily deploy, manage, and scale LLMs without worrying about the underlying infrastructure.

Here are some of the key features of Scaleway Managed Inference:

* **Easy deployment**: Deploy state-of-the-art open weights LLMs with just a few clicks. Scaleway Managed Inference provides a simple and intuitive interface for generating dedicated endpoints.
* **Security**: Scaleway provides [a secure environment](/managed-inference/reference-content/data-privacy-security-scaleway-ai-services/) to run your models. Our platform is built on top of a secure architecture, and we use state-of-the-art cloud security.
* **Complete data privacy**: [No storage](/managed-inference/reference-content/data-privacy-security-scaleway-ai-services/#data-storage-policies) or third-party access to your data (prompt or responses), to ensure it remains exclusively yours.
* **Interoperability**: Scaleway Managed Inference was designed as a drop-in [replacement for the OpenAI APIs](/managed-inference/reference-content/openai-compatibility/), for a seamless transition on your applications already using its libraries.

## Console overview

Discover the Managed Inference interface on the Scaleway console.
<GuideFlow src="https://app.guideflow.com/embed/0p0ozjebvp"/>

<Requirements />

  - A Scaleway account logged into the [console](https://console.scaleway.com)
  - [Owner](/iam/concepts/#owner) status or [IAM permissions](/iam/concepts/#permission) allowing you to perform actions in the intended Organization

## How to create a Managed Inference deployment

1. Navigate to the **AI** section of the [Scaleway console](https://console.scaleway.com/), and select **Managed Inference** from the side menu to access the Managed Inference dashboard.
2. From the drop-down menu, select the geographical region where you want to create your deployment.
3. Click **Create deployment** to launch the deployment creation wizard.
4. Provide the necessary information:
    - Select the desired model and the quantization to use for your deployment [from the available options](/managed-inference/reference-content/).
        <Message type="important">
          Scaleway Managed Inference allows you to deploy various AI models, either from the Scaleway catalog or by importing a custom model. For detailed information about supported models, visit our [Supported models in Managed Inference](/managed-inference/reference-content/supported-models/) documentation.
        </Message>
        <Message type="note">
          Some models may require acceptance of an end-user license agreement (EULA). If prompted, review the terms and conditions and accept the license accordingly.
        </Message>
    - Choose the geographical **region** for the deployment.
    - Select a node type, the GPU Instance that will be used with your deployment.
    - Choose the number of nodes for your deployment. Note that this feature is currently in [Public Beta](https://www.scaleway.com/betas/).
        <Message type="note">
          High availability is only guaranteed with two or more nodes.
        </Message>
5. Enter a **name** for the deployment, along with optional tags to aid in organization.
6. Configure the **network** settings for the deployment:
    - Enable **Private Network** for secure communication and restricted availability within Private Networks. Choose an existing Private Network from the drop-down list, or create a new one.
    - Enable **Public Network** to access resources via the public Internet. API key protection is enabled by default.
    <Message type="important">
      - Enabling both private and public networks will result in two distinct endpoints (public and private) for your deployment.
      - Deployments must have at least one endpoint, either public or private.
    </Message>
7. Click **Create deployment** to launch the deployment process. Once the deployment is ready, it will be listed among your deployments.

## How to access a Managed Inference deployment

Managed Inference deployments have authentication enabled by default. As such, your endpoints expect a secret key generated with Scaleway's Identity and Access Management service (IAM) for authentication.

1. Click **Managed Inference** in the **AI** section of the side menu. The Managed Inference dashboard displays.
2. From the drop-down menu, select the geographical region where you want to manage.
3. Click the name of the deployment you wish to access. The deployment's **Overview** page displays.
4. Scroll down to the **Deployment authentication** section and click the **Generate key** button. The token creation wizard displays.
5. Fill in the [required information for API key creation](/iam/how-to/create-api-keys/) and click **Generate API key**.
6. Copy and safely store your credentials before closing the window, as they will not be shown again.

<Message type="tip">
  You have full control over authentication from the **Security** tab of your deployment. Authentication is enabled by default.
</Message>

## How to interact with Managed Inference

1. Click **Managed Inference** in the **AI** section of the side menu. The Managed Inference dashboard displays.
2. From the drop-down menu, select the geographical region where your desired deployment was created.
3. Click the name of the deployment you wish to edit. The deployment's **Overview** page displays.
4. Click the **Playground** tab, then **View code** to see code examples in various environments. Copy and paste them into your code editor or terminal.

<Message type="note">
  Prompt structure may vary from one model to another. Refer to the specific instructions for use in our [dedicated documentation](/managed-inference/reference-content/).
</Message>

## How to delete a deployment

1. Click **Managed Inference** in the **AI** section of the [Scaleway console](https://console.scaleway.com) side menu. A list of your deployments displays.
2. From the drop-down menu, select the geographical region where your deployment was created.
3. Click the name of the deployment you wish to delete.
4. Navigate to the **Settings** tab.
5. Click **Delete deployment** at the bottom of the page.
6. Type **DELETE** to confirm and click **Delete deployment**.

  Alternatively, from the Deployments listing, click the <Icon name="more" /> icon next to the deployment name you no longer need, and click **Delete**. A pop-up appears. Type **DELETE** to confirm, then click **Delete deployment**.

<Message type="important">
  Deleting a deployment is a permanent action that erases all its associated data and resources.
</Message>