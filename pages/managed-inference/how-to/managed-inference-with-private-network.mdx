---
title: How to use your Managed Inference deployment with a Private Network
description: Learn how to deploy and manage AI models securely using Scaleway's Managed Inference with a private network configuration, ensuring optimal performance and data protection.
tags: managed-inference ai-data private-network
dates:
  validation: 2025-07-16
  posted: 2024-06-18
---
import Requirements from '@macros/iam/requirements.mdx'

import image from './assets/scaleway-inference-pn-connected.webp'
import image2 from './assets/scaleway-inference-tls-dl.webp'
import image3 from './assets/scaleway-inference-pn-detach.webp'


In this tutorial, we guide you through the process of attaching a Private Network to your Managed Inference deployment.
This can be done during the initial setup or added later to an existing deployment.
Using a Private Network for communications between the Instances hosting your applications and your Managed Inference deployment ensures secure communication between resources, with low latency.

<Requirements />

  - A Scaleway account logged into the [console](https://console.scaleway.com)
  - [Owner](/iam/concepts/#owner) status or [IAM permissions](/iam/concepts/#permission) allowing you to perform actions in the intended Organization
  - A [Managed Inference deployment](/managed-inference/quickstart/)


## How to attach a Private Network to a Managed Inference deployment

### Attaching a Private Network during model deployment

1. Click **Managed Inference** in the **AI** section of the [Scaleway console](https://console.scaleway.com) side menu. A list of your deployments displays.
2. From the drop-down menu, select the geographical region where you want to deploy.
3. Navigate to the **Deployments** section and click **Deploy a model**. The setup wizard displays.
4. During the [setup](/managed-inference/how-to/create-deployment/), in the step **Configure network connectivity**, check the **Attach to Private Network** box.
5. Choose one of the following options:
    - **Attach an existing Private Network**: Select from the list of available networks.
    - **Create a new Private Network**: Choose this option (at the bottom of the list) to create a new Private Network to attach the model to.
6. *Complete the deployment setup process and click **Deploy model**.

Your Managed Inference model will be deployed, and it will be attached to the selected Private Network.

### Attaching a Private Network to an existing deployment

1. Click **Managed Inference** in the **AI** section of the [Scaleway console](https://console.scaleway.com) side menu. A list of your deployments displays.
2. From the drop-down menu, select the geographical region you want to manage.
3. Click a deployment name or <Icon name="more" /> > **More info** to access the deployment dashboard.
4. Go to the **Overview** tab and locate the **Endpoints** section.
5. Click **Attach Private Network**. Two options are available:
    - **Attach an existing Private Network**: Select from the list of available networks.
    - **Create a new Private Network**: Choose this option from the end of the list to create a new Private Network to attach the model to.
    <Message type="tip">
      Alternatively, you can access the **Security tab** and attach a network from the **Private Network** section.
    </Message>
6. Click **Attach to Private Network** to confirm.

### Verifying the Private Network connection

1. After attaching a Private Network, go to the **Security** tab.
2. You should see the **Private Network** connected to the deployment resource and its **allocated IPs** listed.
    <Lightbox image={image} alt="A screenshot of the Scaleway console, highlighting the Private Network configuration of the Managed Inference deployment" size="medium" />

## How to send inference requests in a Private Network

<Message type="note">
For more information on managing access to deployments in a Private Network, see [How to manage access to deployments](/managed-inference/how-to/manage-allowed-ips/).
</Message>

1. [Create an Instance](/instances/how-to/create-an-instance/) which will host the inference application.
    <Message type="important">
      Ensure the Instance is attached to a Private Network in the same VPC as your Managed Inference deployment.
    </Message>
2. Download the TLS certificate from your Managed Inference deployment, available from the **Overview** tab in the **Endpoints** section.
    <Lightbox image={image2} alt="A screenshot of the Managed Interface product overview tab in the Scaleway console, highlighting the TLS certificate download section" size="medium" />
3. Transfer the TLS certificate to the Instance. You can use the `scp` (secure copy) command to securely transfer the certificate from your local machine to the Scaleway Instance.
   - Example command:
     ```bash
     scp -i ~/.ssh/id_ed25519 /home/user/certs/cert_file.pem root@51.115.xxx.yyy:/root
     ```
   - Replace placeholders in the command above as follows:
     - `-i ~/.ssh/id_ed25519`: Path to your private SSH key.
     - `/home/user/certs/cert_file.pem`: Path to the certificate file on your local machine.
     - `root`: Your Scaleway Instance username (`root` for default configuration).
     - `51.115.xxx.yyy`: Public IP address of your Scaleway Instance.
     - `:/root`: Destination directory on the Scaleway Instance.
    <Message type="note">
      Ensure the destination directory exists and you have write permissions. Create the directory if necessary.
    </Message>
4. Connect to your Instance [using SSH](/instances/how-to/connect-to-instance/).
5. Open a text editor and create a file named `inference.py` using the following command:
     ```bash
     nano inference.py
     ```
6. Paste the following Python code sample into your `inference.py` file:
    <Message type="tip">
    This script takes an example of a conversational task with a request sent to an [LLM through the Chat Completions API](/managed-inference/reference-content/openai-compatibility/#chat-completions-api).
    </Message>
    ```py
    import requests

    PAYLOAD = {
        "model": "<MODEL_DEPLOYED>", # EXAMPLE= meta/llama-3.1-8b-instruct:fp8
        "messages": [
            {"role": "system",
            "content": "You are a helpful, respectful and honest assistant."},
            {"role": "user",
            "content": "How can I use large language models to improve customer service?"}
        ],
        "max_tokens": 500,
        "temperature": 0.7,
        "stream": False
    }

    headers = {"Authorization": "Bearer " + "<SCW_SECRET_KEY>"} # ADD API KEY, IF API KEY AUTHENTICATION IS ENABLED FOR THE PRIVATE ENDPOINT

    response = requests.post("<PRIVATE_ENDPOINT_URL>/v1/chat/completions",
                            headers=headers, json=PAYLOAD, stream=False, verify='<CERT_NAME>.pem')

    if response.status_code == requests.codes.ok:
        # EXTRACT RESPONSE DATA
        data = response.json()
        content = data['choices'][0]['message']['content']
        print(content)
    else:
        print("Error occurred:", response.text)
    ```
   Edit the script as follows:
     - **PAYLOAD**: Update the model name and inference parameters.
     - **headers**: Add your IAM secret key if IAM authentication is enabled.
     - **response**: Update with your private endpoint URL and certificate file path.

7. Save your changes using `CONTROL+O`, then exit with `CONTROL+X`.
8. Make your script executable using the following command:
     ```bash
     chmod +x inference.py
     ```

9. Run the script:
     ```bash
     python3 inference.py
     ```

## Detaching a Private Network from a Managed Inference deployment

1. Click **Managed Inference** in the **AI** section of the [Scaleway console](https://console.scaleway.com) side menu. A list of your deployments displays.
2. Click a deployment name or <Icon name="more" /> > **More info** to access the deployment dashboard.
3. Go to the **Overview** tab and locate the **Endpoints** section.
4. Click **Detach Private Network**. A pop-up displays.
    <Lightbox image={image3} alt="A screenshot of the Managed Interface product overview tab in the Scaleway console, highlighting the Private Network detach section" size="medium" />
5. Click **Detach resource** to confirm the removal of the private endpoint for your deployment.
    <Message type="tip">
      Alternatively, you can access the **Security** tab and detach a network from the **Private Network** section.
    </Message>
    <Message type="important">
      Managed Inference deployments must have at least one endpoint, either public or private. Consequently, users cannot detach their private endpoint without having a public one.
    </Message>
