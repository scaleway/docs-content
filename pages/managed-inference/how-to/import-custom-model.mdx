---
meta:
  title: How to import custom models into Managed Inference
  description: Learn how to import your custom models into Scaleway's Managed Inference platform.
content:
  h1: How to import custom models into Managed Inference
  paragraph: Learn how to import your custom models into Scaleway's Managed Inference platform.
tags: managed-inference ai-data import custom model
dates:
  validation: 2025-03-27
  posted: 2025-03-27
categories:
  - ai-data
---

Scaleway provides a selection of common models for deployment from the Scaleway console. If you need a specific model, you can import it into Managed Inference directly from Hugging Face or a Scaleway Object Storage bucket.

<Message type="note">
  This feature is currently in **beta stage** and will evolve in the future.
</Message>

<Macro id="requirements" />
- A Scaleway account logged into the [console](https://console.scaleway.com).
- [Owner](/iam/concepts/#owner) status or [IAM permissions](/iam/concepts/#permission) to perform actions in your Organization.

1. Click **Managed Inference** in the **AI** section of the side menu in the [Scaleway console](https://console.scaleway.com/) to access the dashboard.
2. Click **Deploy a model** to launch the model deployment wizard.
3. In the **Choose a model** section, select **Custom model**. If you have no model yet, click **Import a model** to start the model import wizard.
4. Choose an upload source:
   - **Hugging Face**: Pull the model from Hugging Face.
   - **Object Storage**: This feature is coming soon.
5. Enter your Hugging Face access token, which must have READ access to the repository.
    <Message type="note">
      [Learn how to generate a Hugging Face access token](https://huggingface.co/docs/hub/security-tokens).
    </Message>
6. Enter the name of the Hugging Face repository to pull the model from.
    <Message type="note">
      Ensure you have access to gated models if applicable. Refer to the [Hugging Face documentation](https://huggingface.co/docs/hub/en/models-gated) for details.
    </Message>
7. Choose a name for your model. The name must be unique within your Organization and Project and cannot be changed later.
8. Click **Verify import** to check your Hugging Face credentials and ensure model compatibility.
    <Message type="tip">
      For detailed information about supported models, visit our [Supported models in Managed Inference](/managed-inference/reference-content/supported-models/) documentation.
    </Message>
9. Review the summary of your import, which includes:
    - Context size by node type.
    - Quantization options.
    - Estimated cost.
   Once checked, click **Begin import** to finalize the process.
   <Message type="note">
      Importing a model may take some time, depending on its size. Once the import is complete, the model will appear in the model library, and you will be able to use it for deployment.
   </Message>

Your imported model will now appear in the model library and you can [deploy it on Managed Inference](/managed-inference/how-to/create-deployment/).

