---
title: GPU Instances - Quickstart
description: This page shows you how to get started with Scaleway GPU Instances.
tags: instance gpu-instance
dates:
  validation: 2026-01-29
  posted: 2022-03-25
---
import Requirements from '@macros/iam/requirements.mdx'

import image from './assets/scaleway-jupyter-lab.webp'

## Console overview
Discover the GPU Instances interface on the Scaleway console.
<GuideFlow src="https://app.guideflow.com/embed/dr9mnv2snp"/>

<Requirements />

Scaleway GPU Instances are virtual compute instances equipped with dedicated high-end Nvidia graphics processing units (GPUs). They are ideal for data processing, artificial intelligence, rendering, and video encoding. After you have created your GPU Instance, you can [connect to it via SSH](/instances/how-to/connect-to-instance/) and run one of our ready-made [Docker images](/gpu/how-to/use-gpu-with-docker/) to access a [preinstalled environment](/gpu/how-to/use-preinstalled-env/) with all your [favorite AI libraries and tools](/gpu/reference-content/docker-images/) preinstalled.

In addition to this, GPU Instances have all the features of our regular Instances, including [flexible IPs](/instances/how-to/use-flexips/), [security groups](/instances/how-to/use-security-groups/), [Private Networks](/instances/how-to/use-private-networks/), [backups](/instances/how-to/create-a-backup/) and more. When you are done using your GPU Instance, you can easily [delete it](/instances/how-to/delete-instance/) from the Scaleway console.

## How to create a GPU Instance

1. Click **CPU & GPU Instances** in the **Compute** section of the side menu. The [Instance dashboard](https://console.scaleway.com/instance/servers) displays.
2. Click **Create GPU Instance**, then select **Create CPU Instance**. The Instance creation wizard displays.
3. Select an Instance range based on your usage. Instance offers vary in pricing, processing power, memory, storage, and bandwidth. [Discover the best Instance type for your needs](/instances/reference-content/choosing-instance-type/).
    <Message type="tip">
      Click **Instance view** > **List** to view a list of all available Instances.
    </Message>
4. Choose an Instance configuration from the list and click the select icon. A pop-up displays.
5. Select the Availability Zone in which you want to deploy your Instance. Then click **Configure Instance**.
6. Complete the following steps:
    - **Name your Instance**, or leave the randomly-generated name in place. Optionally, you can add [tags](/instances/concepts/#tags) to help you organize your Instance.
    - **Image**: Pick an operating system image suitable for your GPU Instance. For example, select **Ubuntu Noble GPU OS 13 (NVIDIA)**, which comes with preinstalled NVIDIA drivers and an NVIDIA Docker environment. You can customize your working environment using Docker with our provided Docker images or your own containers.
    - **Add volumes**, which are storage spaces used by your Instances. A block volume with a default name and 5,000 IOPS is automatically provided for your system volume. You can customize this volume and attach up to 16 local and/or block type volumes as needed.
      <Message type="important">
          - Ensure that the volume containing your OS image has a minimum size of 10 GB. For a GPU OS, the recommended size is 125 GB.
          - When multiple Block Storage volumes are linked to your Instance, the primary volume will host the OS and is essential for booting the Instance. If needed, you can [change the boot volume after the Instance creation](/instances/how-to/use-boot-modes/#how-to-change-the-boot-volume).
          - Booting from a volume that either lacks an OS or is among multiple volumes with identical operating systems can lead to inconsistent boot outcomes.
      </Message>
    - **Configure network** of the Instance.
      - Leave the checkbox ticked to assign a **Public IPv4** to the Instance. You can either allocate a new IPv4 address or select one or multiple existing IPv4s. Alternatively, uncheck the box if you do not want an IPv4.
      - Leave the checkbox ticked to assign a **Public IPv6** to the Instance. You can either allocate a new IPv6 address or select one or multiple existing IPv6s. Alternatively, uncheck the box if you do not want an IPv6.
        <Message type="note">
          You can attach up to 5 IPv4 and 5 IPv6 addresses to an Instance, which is useful for running different services or applications on the same Instance.
        </Message>
    - (Optional) Click **Cloud-init** to set up a [cloud-init configuration](/instances/concepts/#cloud-init). Otherwise, leave this option set to its default values.
      Cloud-init scripts are the standard way to automate Instance setup, such as setting up software, users, and system configurations at the first boot.
    - **Verify the [SSH keys](/organizations-and-projects/concepts/#ssh-key)** that will give you access to your Instance.
7. Click **Review** to see a order summary. Verify the estimated cost of your Instance, based on the specifications you chose.
8. Click **Create Instance**. The creation of your Instance begins, and you will be informed when the Instance is ready.


## How to connect to your GPU Instance

See our documentation on [how to connect to your Instance via SSH](/instances/how-to/connect-to-instance/).

## How to launch a Docker container

To access a preinstalled working environment with all your favorite Python packages, you need to launch a Docker container.

<Message type="tip">
  If you created your GPU Instance with one of the legacy Ubuntu Bionic ML OS images, once you connect to your Instance you are already in a preinstalled ready-to-use Python environment, managed with conda. You do not need to follow the steps for launching a Docker container.
</Message>

1. Choose one of our [Docker AI images](/gpu/reference-content/docker-images/) (eg Tensorflox, Pytorch, Jax) based on your needs.
2. Run the following command to launch the Docker container. In the following example, we launch a container based on the **Tensorflow** image:
    ```bash
    docker run --runtime=nvidia -it --rm -p 8888:8888 -p 6006:6006 rg.fr-par.scw.cloud/scw-ai/tensorflow:latest /bin/bash
    ```

## How to use the preinstalled environment

When you run your Docker container as shown above, the container launches and you are taken to its `ai` directory, where the Pipenv virtual environment is already activated.

<Message type="tip">
  Use the command `pipenv graph` to see a list of all installed packages and their versions, as well as all the dependencies of each package. For more help with pipenv, see our [dedicated documentation](/gpu/how-to/use-pipenv/).
</Message>

### Launching an application in your local browser

Some applications, such as [Jupyter Lab](https://jupyter.org/), [Tensorboard](https://www.tensorflow.org/tensorboard) and [Code Server](https://code-server.dev/), require a browser to run. You can launch these from the `ai` virtual environment of your Docker container, and view them in the browser of your local machine. This is possible thanks to the addition of port mapping arguments when launching a container with the `docker run` command. In our example, we added the port mapping arguments `-p 8888:8888 -p 6006:6006` when we launched our container, mapping `8888:8888` for Jupyter Lab and `6006:6006` for Tensorboard.

<Message type="tip">
  Code Server runs in Jupyter Lab via Jupyter Hub and does not need port mapping in this case. You can add other port mapping arguments for other applications as you wish.
</Message>

1. Launch an application. Here, we launch Jupyter Lab:

    `jupyter-lab`

    Within the output, you should see something similar to the following:

    ```
    [I 2022-04-06 11:38:40.554 ServerApp] Serving notebooks from local directory: /home/jovyan/ai
    [I 2022-04-06 11:38:40.554 ServerApp] Jupyter Server 1.15.6 is running at:
    [I 2022-04-06 11:38:40.554 ServerApp] http://7d783f7cf615:8888/lab?token=e0c21db2665ac58c3cf124abf43927a9d27a811449cb356b
    [I 2022-04-06 11:38:40.555 ServerApp]  or http://127.0.0.1:8888/lab?token=e0c21db2665ac58c3cf124abf43927a9d27a811449cb356b
    [I 2022-04-06 11:38:40.555 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
    ```

    <Message type="tip">
      Jupyter Lab is launched automatically when you run any Scaleway container image. You will see a message upon start up telling how to access the notebook in your browser. To override Jupyter Lab being launched automatically in this way, add `/bin/bash` to the end of your `docker run` command, e.g. `docker run --runtime=nvidia -it --rm -p 8888:8888 -p 6006:6006 rg.fr-par.scw.cloud/scw-ai/pytorch:latest /bin/bash`. This preempts the launch of Jupyter Lab at container startup and replaces it with the specified command. In this case, a bash shell.
    </Message>
2. On your local computer, open a browser window and enter the following URL. Replace `<ip-address>` with the IP address of your Scaleway GPU Instance, and `<my-token>` with the token shown displayed in the last lines of terminal output after the `jupyter-lab` command
    ```
    <ip-address>:8888/lab?token=<my-token>
    ```

    <Message type="tip">
      You can find the IP address of your Instance in the Scaleway console. In the side menu, click **CPU & GPU Instances** to see a list of your Instances. The IP address of each of them is shown in the list that displays.
    </Message>

    Jupyter Lab now displays in your browser. You can use the Notebook, Console, or other features as required:

    <Lightbox image={image} alt="" />

    You can display the GPU Dashboard in Jupyter Lab to view information about CPU and GPU resource usage. This is accessed via the **System Dashboards** icon on the left side menu (third icon from the top).
3. Use CTRL+C in the terminal window of your GPU Instance / Docker container to close down the Jupyter server when you have finished.

## Exiting the preinstalled environment and the Docker container

When you are in the activated Pipenv virtual environment, your command line prompt will normally be prefixed by the name of the environment. Here, for example, from `(ai) jovyan@d73f1fa6bf4d:~/ai` we see that we are in the activated `ai` environment, and from `jovyan@d73f1fa6bf4d:~/ai` that we are in the `~/ai` directory of our container:

1. Type `exit` the following command to leave the preinstalled `ai` environment.

    You are now outside the preinstalled virtual environment.
2. Type `exit` again to exit the Docker container.

    Your prompt should now look similar to the following. You are still connected to your GPU Instance, but you have left the Docker container:

    ```
    root@scw-name-of-instance:~#
    ```
3. Type `exit` once more to disconnect from your GPU Instance.