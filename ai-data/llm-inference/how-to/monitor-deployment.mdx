---
meta:
  title: How to monitor a LLM Inference deployment
  description: This page explains how to monitore a LLM Inference deployment
content:
  h1: How to monitor a LLM Inference deployment
  paragraph: This page explains how to monitore a LLM Inference deployment
tags: ai monitoring
dates:
  validation: 2024-03-06
  posted: 2024-03-06
categories:
  - ai-data
---

Monitoring allows you to monitor your deployment.

<Macro id="iam-requirements" />

<Message type="requirement">
  - You have an account and are logged into the [Scaleway console](https://console.scaleway.com)
  - You have a [LLM Inference deployment](/ai-data/llm-inference/quickstart/)
  - You have [enabled your Cockpit](/observability/cockpit/how-to/activate-cockpit/)
</Message>


## How to monitor your LLM dashboard

1. Click **LLM Inference** in the **AI & Data** section of the [Scaleway console](https://console.scaleway.com) side menu. A list of your deployments displays.
2. Click a deployment name or <Icon name="more" /> > **More info** to access the deployment dasdhboard.
3. Click the **Monitoring** tab of your deployment. The Cockpit overview displays.
4. Click **Open Grafana metrics dashboard** to open your Cockpit's Grafana interface.
5. Authenticate with your Grafana credentials. The Grafana dashboard displays.
6. Select your LLM Inference dashboard from the [list of your managed dashboards](/observability/cockpit/how-to/access-grafana-and-managed-dashboards/) to visualize your metrics.