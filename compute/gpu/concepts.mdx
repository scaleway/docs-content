---
meta:
  title: GPU Instances - Concepts
  description: This page explains all the concepts related to GPU Instances
content:
  h1: GPU Instances - Concepts
  paragraph: This page explains all the concepts related to GPU Instances
tags: instance gpu gpu-instance docker nvidia pipenv
categories:
  - compute
---

<Message type="note">
  GPU Instances share many concepts in common with traditional Instances. See also the [Instances Concepts page](/compute/instances/concepts/) for more information.
</Message>

<Concept opened>

## AI-Enabled applications

The proliferation of AI-Enabled applications and innovative use cases is poised to bring transformative changes to various domains, including business, social interactions, and human-machine interfaces.
These applications will feature conversational and multimodal user interfaces (UIs) that will revolutionize interactions within smart spaces, smart robots, and autonomous vehicles.

</Concept>

<Concept opened>

## Automatic Mixed Precision

In the realm of deep neural network training, the standard approach has been to use the IEEE single-precision format.
Nevertheless, an alternative method called mixed precision has emerged, allowing training with half precision while retaining the network's accuracy achieved with single precision.
This innovative technique, known as mixed-precision, combines both single- and half-precision representations.
Major AI frameworks (such as TensorFlow and PyTorch) have embraced this approach, offering automatic mixed precision support to accelerate AI training on NVIDIA GPUs with minimal code adjustments.

</Concept>

<Concept>

## CUDA

CUDA is a parallel computing platform and API model created by NVIDIA. CUDA is an acronym for **c**ompute **u**nified **d**evice **a**rchitecture.

</Concept>

<Concept>

## Data-Centric AI

Data-Centric AI delves into diverse data and analytics techniques, aiming to derive insights from business, Internet of Things (IoT), and sensor data.
Additionally, it seeks to enhance AI-enabled decisions, making them not only more accurate but also explainable and ethical.
In this context, the trustworthiness of AI systems, which operate with varying degrees of autonomy, is essential, and their risks need to be effectively managed.

</Concept>

<Concept>

## Docker

[Docker](https://www.docker.com/) is a platform as a service (PaaS) tool, that uses OS-level virtualization to deliver software in packages called containers. Scaleway provides a number of pre-built Docker images, which allow you to [run a Docker container on your GPU Instance](/compute/gpu/how-to/use-gpu-with-docker/) and access a pre-installed Pipenv environment ideal for your AI projects.

</Concept>


<Concept>

## Generative AI

Generative AI involves AI methods that learn representations from data and utilize them to create entirely new artifacts while retaining similarities to the original data.
These artifacts can be applied for both positive and negative purposes.
Generative AI has the ability to generate fresh media content, such as text, images, videos, and audio, as well as synthetic data and models of physical objects. 
Additionally, it finds applications in fields like drug discovery and material design.

</Concept>

<Concept>

## GPU

**G**raphical **P**rocessing **U**nit (GPU) became a go-to term for the specialized electronic circuits, designed to power graphics on a machine. The term was popularized in the late 1990s by the chip manufacturer NVIDIA. GPUs were originally produced primarily to drive high-quality gaming experiences, producing life-like digital graphics. Today, those capabilities are being harnessed more broadly for data processing, articificial intelligence, rendering and video encoding.

</Concept>


<Concept>

## Model-Centric AI

Model-Centric AI focuses on the most promising and emerging techniques that will pave the way for future groundbreaking advancements in the field of AI.
These techniques include foundation models, composite AI, physics-informed AI, neuromorphic computing, and biology-inspired algorithms.
By leveraging these innovative approaches, Model-Centric AI aims to drive revolutionary progress in the world of artificial intelligence.

</Concept>

<Concept>

## Nvidia

Nvidia is a brand of GPU for the gaming and professional markets. Scaleway GPU Instances are equipped with NVidia GPUs.

</Concept>


<Concept>

## Inference and Training

At a high level, working with deep neural networks involves a two-stage process.
First, the neural network undergoes training, where its parameters are determined by using labeled examples of inputs and corresponding desired outputs.
Then, the trained network is deployed for inference, utilizing the learned parameters to classify, recognize, and process unfamiliar inputs.

</Concept>

<Concept>

## Moore's Law

Moore's law, originally observed by Intel co-founder Gordon Moore in 1965, describes a consistent trend wherein the number of transistors per square inch on integrated circuits doubles every year since their inception.
This observation led to the prediction that this trend would persist into the foreseeable future.
However, over the past decade, Moore's law has shown signs of deceleration, mainly due to the rapid advancement of GPU architectures, which have outpaced the rate of CPU performance-per-watt growth.

</Concept>

<Concept>

## MXNet

MXNet is a modern, open-source deep learning framework employed for the training and deployment of deep neural networks.
It provides scalability, facilitating swift model training, while also supporting a versatile programming model and multiple languages. 

</Concept>

<Concept>

## PetaFLOPS

PetaFLOPS is a unit of computing speed equal to one thousand million million (10^15) floating-point operations per second.

</Concept>

<Concept>

## Pipenv

Pipenv is a package and dependency manager for Python projects. It harnesses the power of different existing tools, bringing their functionalities together:

- `pip` for Python package management
- `pyenv` for Python version management
- `Virtualenv` for creating different virtual Python environments
- `Pipfile` for managing project dependencies

Pipenv is pre-installed on all of Scaleway's AI Docker images for GPU Instances, making it easy to [use virtual environments](/compute/gpu/how-to/use-pipenv/). Pipenv replaces Anaconda for this purpose.

</Concept>

<Concept>

## PyTorch

PyTorch is a GPU-accelerated tensor computational framework with a Python front end.
It provides a seamless integration with popular Python libraries like NumPy, SciPy, and Cython, allowing for easy extension of its functionality.

</Concept>

<Concept>

## ResNet, ResNet-50

ResNets, short for deep residual networks, were created by Microsoft Research for Image Recognition tasks.
These networks achieved remarkable success, securing first place in all five main tracks of the ImageNet and COCO 2015 competitions, encompassing image classification, object detection, and semantic segmentation.
Over time, ResNets have demonstrated their robustness through various visual recognition tasks and even extended their effectiveness to non-visual tasks involving speech and language.

</Concept>

<Concept>

## Responsible and Human-Centric AI

Responsible and Human-Centric AI emphasizes the positive impact of AI on individuals and society, while also addressing the need to manage and mitigate AI-related risks.
It encourages vendors to adopt ethical and responsible practices in their AI implementations.
Additionally, it advocates for combining AI with a human touch and common sense to ensure a more holistic and beneficial AI experience for everyone.

</Concept>

<Concept>

## Scratch Storage

Scratch Storage is temporary local storage that operates differently from classic local storage.
It does not support snapshots, backup, or restore features. Additionally, you cannot download an image into it as it is intended solely for caching purposes.

</Close>

<Concept>

## Structural Sparsity

Modern AI networks are characterized by their substantial size, containing millions or even billions of parameters.
However, not all these parameters are indispensable for precise predictions, and certain ones can be transformed into zeros, resulting in "sparse" models that maintain accuracy.
Although the sparsity feature in the A100 and H100 GPUs primarily enhances AI inference, it also contributes to improved model training performance.

</Concept>

<Concept>

## TensorFlow

TensorFlow is an open-source machine learning software library designed to address a diverse array of tasks.
Initially developed by Google, it caters to their requirements for creating and training neural networks capable of detecting and interpreting patterns and correlations, akin to human learning and reasoning processes.

</Concept>

<Concept>

## Tensor Cores

NVIDIA's V100, A100, H100, L4, and L40 GPUs are equipped with Tensor Cores, which significantly enhance the performance of matrix-multiplication operations, crucial for neural network training and inferencing.
These specially designed Tensor Cores and their associated data paths are engineered to substantially increase floating-point compute throughput while incurring only modest area and power costs.

</Concept>

<Concept>

## TeraFLOPS

TeraFLOPS is a unit of computing power equal to one million (10^12) floating-point operations per second.

</Concept>

<Concept>

## TF32

TF32 is a new precision that functions similarly to FP32. It provides remarkable speed improvements of up to 10X for AI tasks, all without necessitating any alterations to the existing code.

</Concept>