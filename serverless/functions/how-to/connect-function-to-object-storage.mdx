---
meta:
  title: How to connect a Serverless Function to Object Storage
  description: This page explains how to connect a Serverless Function to Object Storage
content:
  h1: How to connect a Serverless Function to Object Storage
  paragraph: This page explains how to connect a Serverless Function to Object Storage
tags: functions serverless object-storage serverless-function boto3 sdk 
dates:
  validation: 2023-01-02
  posted: 2021-05-26
categories:
  - serverless
---

Because Serverless functions are stateless, you will likely need to connect to services such as [Object Storage](https://www.scaleway.com/en/object-storage/) or [Databases](https://www.scaleway.com/en/database/) to store your data.
This page shows an example of how to connect to Scaleway's Object Storage using the AWS SDK and Python or Node.

<Macro id="iam-requirements" />

<Message type="requirement">
  - You have an account and are logged into the [Scaleway console](https://console.scaleway.com)
  - You have created a [functions namespace](/serverless/functions/how-to/create-a-functions-namespace/)
  - You have [generated an API key](/identity-and-access-management/iam/how-to/create-api-keys/)
</Message>

## Environment variables

Both our Python and Node code examples below require you to add the following environment variables:
    * `ACCESS_KEY_ID` (the access key of your [Scaleway API key](/identity-and-access-management/iam/how-to/create-api-keys/))
    * `ACCESS_KEY` (the secret key of your [Scaleway API key](/identity-and-access-management/iam/how-to/create-api-keys/)
    * `BUCKET_NAME` (the name for your bucket)
    * `S3_ENDPOINT_URL` (the [bucket endpoint URL](/storage/object/concepts/#endpoint))

<Message type="iam">
 You can use a Scaleway API key attached to an IAM application in the `<ACCESS_KEY_ID>` and `<ACCESS_KEY>` fields. Make sure that the IAM application has a [policy](/identity-and-access-management/iam/concepts/#policy) giving it permissions to read and create Object Storage buckets, and that the API key's [preferred project for Object Storage](/identity-and-access-management/iam/api-cli/using-api-key-object-storage/) is set to the correct Project.
</Message>


## Python

We are going to use the AWS SDK for Python (boto3) in the following steps. You can run these steps on your local computer or any Linux Instance. [Learn more about boto3.](/storage/object/api-cli/object-storage-aws-cli/)

1. Download boto3 in the package folder:
    ```
    pip install boto3 --target ./package
    ```
    <Message type="note">
    Alternatively you can use a requirements.txt file:
    ```
    pip install -r requirements.txt --target ./package
    ```

    </Message>

2. Write your code.
    * Read a bucket object:
    ```py
    import boto3
    from botocore.exceptions import ClientError
    import os

    # Defining bucket from environment variables
    bucket = os.environ.get('BUCKET_NAME')
    region_name=os.environ.get('REGION')
    endpoint_url=os.environ.get('S3_ENDPOINT_URL')
    aws_access_key_id=os.environ.get('ACCESS_KEY_ID')
    aws_secret_access_key=os.environ.get('ACCESS_KEY')

    # Create S3 client
    s3 = boto3.resource(
        's3',
        region_name=region_name,
        use_ssl=True,
        endpoint_url=endpoint_url,
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key
    )

    def handle(event, context):
        # Get object from S3
        obj = s3.Object(bucket, <file)
        
        # Read object content
        body = obj.get()['Body'].read().decode('utf-8') #be careful to choose the right encoding
        
        return {
            "body": { "Object Body": body
            },
            "statusCode": 200,
        }
    ```

    * Write a new bucket object:
    ```py
    import boto3
    from botocore.exceptions import ClientError
    import os

    # Defining bucket form environment variables
    bucket = os.environ.get('BUCKET_NAME')
    region_name=os.environ.get('REGION')
    use_ssl=True
    endpoint_url=os.environ.get('S3_ENDPOINT_URL')
    aws_access_key_id=os.environ.get('ACCESS_KEY_ID')
    aws_secret_access_key=os.environ.get('ACCESS_KEY')

    # Create S3 client
    s3 = boto3.resource(
        's3',
        region_name=region_name,
        use_ssl=True,
        endpoint_url=endpoint_url,
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key
    )

    def handle(event, context):
        txt_data = b'Hello World!'
        try:
            # Create object on S3 and write data
            obj = s3.Object(bucket, <File Name>)
            obj.put(
                Body=txt_data
            )
            return {
                "body": {"result": "New file created"},
                "statusCode": 200,
            }
        # Handling errors
        except ClientError as e:
            print("Error while writing file: ", e)
            return {
                "body": {"result": "Error couldn't connect to Object Storage"},
                "statusCode": 500,
            }
    ```
3. Push your code to Serverless Function [using a zip-file](/serverless/functions/how-to/package-function-dependencies-in-zip/) or the serverless framework (the latter is recommended).
    * Do not forget to add the following environment variables:
        * `ACCESS_KEY_ID`
        * `ACCESS_KEY`
        * `BUCKET_NAME`
        * `REGION`
        * `S3_ENDPOINT_URL`
        For more information on boto3, consult the [official documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html). 

## NodeJS

We use the NodeJS aws-sdk library to push a file to an Object storage bucket using the S3 protocol. You can run these steps on your local computer or any Linux Instance.

1. Download the aws-sdk:
    ```sh
    npm i aws-sdk
    ```
2. Write your code.
    * Read a bucket object: 
    ```js
    const AWS = require ('aws-sdk');

    const BUCKET_NAME = process.env.BUCKET_NAME;
    const S3_ENDPOINT_URL= process.env.S3_ENDPOINT_URL;
    const ID = process.env.ACCESS_KEY_ID;
    const SECRET = process.env.ACCESS_KEY;

    // Create S3 service object
    const s3 = new AWS.S3({
        endpoint: S3_ENDPOINT_URL,
        credentials: {
            accessKeyId: ID,
            secretAccessKey: SECRET,
        }
    });

    // Create handler
    module.exports.handle = async (event, context, callback) => {
        const params = {
            Bucket: BUCKET_NAME,
            Key: 'functions.txt'
        };

        try {   
            const data = await s3.getObject(params).promise();
            console.log(`Successfully read file from ${BUCKET_NAME}`);
            const objectData = data.Body.toString('utf-8');
            return {
                statusCode: 200,
                body: JSON.stringify({
                    message: objectData
                }),
                headers: {
                    "Content-Type": "application/json",
                },
            };
        } catch (e) {
            return {
                statusCode: 500,
                body: JSON.stringify({
                    message: `Could not retrieve file from S3: ${e.message}`
                }),
                headers: {
                    "Content-Type": "application/json",
                }
            };
        };

    };
    ```
    * Write to a bucket object:
    ```js

    const AWS = require ('aws-sdk');

    const BUCKET_NAME = process.env.BUCKET_NAME;
    const S3_ENDPOINT_URL= process.env.S3_ENDPOINT_URL;
    const ID = process.env.ACCESS_KEY_ID;
    const SECRET = process.env.ACCESS_KEY;

    // Create S3 service object
    const s3 = new AWS.S3({
        endpoint: S3_ENDPOINT_URL,
        credentials: {
            accessKeyId: ID,
            secretAccessKey: SECRET,
        }
    });

    module.exports.handle = async (event, context, callback) => { 
        const params = {
            Bucket: BUCKET_NAME,
            Key: 'nodejs18file.txt',
            Body: "Hello World from NodeJs 18"
        };

        try {
            const putResult = await s3.putObject(params).promise();
            message=`File uploaded successfully at ${BUCKET_NAME}`;
        } catch (error) { 
            console.log(error);
            return {
                statusCode: 500,
                body: JSON.stringify({
                    message: "Error while uploading file, check logs for more information",
                }),
                headers: {
                    "Content-Type": "application/json",
                },
            };
        }

        return {
            statusCode: 200,
            body: JSON.stringify({
                message: message,
            }),
            headers: {
                "Content-Type": "application/json",
            },
        };
    }
    ```
3. Push your code to Serverless Function [using a zip-file](/serverless/functions/how-to/package-function-dependencies-in-zip/) or the serverless framework (the latter is recommended).

For more information on aws-sdk, consult the [official documentation](https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/s3-example-creating-buckets.html).

## Go

To upload form-data to S3 with a Golang Serverless Function, [check out this code example](https://github.com/scaleway/serverless-examples/tree/main/functions/go-upload-file-s3-multipart).

<Navigation title="See Also">
  <PreviousButton to="/serverless/functions/how-to/connect-function-postgresql-database/">How to connect a function to a PostgreSQL database</PreviousButton>
  <NextButton to="/serverless/functions/how-to/create-auth-token-from-console/">How to create and manage an authentication token from the console</NextButton>
</Navigation>