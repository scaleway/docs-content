---
meta:
  title: Setting up Ampere-optimized AI frameworks on Scaleway COP-ARM Instances
  description: Discover how to set up Ampere-optimized AI frameworks on Scaleway COP-ARM Instances.
content:
  h1: Setting up Ampere-optimized AI frameworks on Scaleway COP-ARM Virtual Instances
  paragraph: Discover how to set up Ampere-optimized AI frameworks on Scaleway COP-ARM Instances.
tags: arm ai ampere pytorch onnx tensorflow
categories:
  - instances
dates:
  validation: 2024-08-12
  posted: 2024-02-09
---

Ampere AI frameworks provide a comprehensive set of tools and libraries for building and deploying AI models on ARM-based Instances.

With the increasing popularity of ARM architecture in data centers and cloud environments, these frameworks can lead to improved performance and efficiency for AI workloads.

In this tutorial, we will walk through the process of setting up and using Ampere-optimized AI frameworks on [Scaleway COP-ARM Instances](https://www.scaleway.com/en/cost-optimized-instances-based-on-arm/).

<Macro id="requirements" />

- A Scaleway account logged into the [console](https://console.scaleway.com)
- [Owner](/identity-and-access-management/iam/concepts/#owner) status or [IAM permissions](/identity-and-access-management/iam/concepts/#permission) allowing you to perform actions in the intended Organization
- An [SSH key](/identity-and-access-management/organizations-and-projects/how-to/create-ssh-key/)

## Setting up the COP-ARM Instance

1. Navigate to your [Scaleway console](https://console.scaleway.com/). Once logged in, click **Instances**, then select **Create Instance** to get started.
2. Choose your preferred Availability Zone.
    <Message type="note">
      Currently, COP-ARM Instances are exclusively available in the **PARIS2** zone.
    </Message>

3. Select the **ARM64** architecture in the drop-down, then select the COP-ARM offer that best fits your requirements.
    <Message type="tip">
        Make sure your Instance has enough resources (CPU, RAM, etc.) for running AI workloads effectively.
    </Message>
4. Select the **Ubuntu 22.04 Jammy Jellyfish** image as your operating system.
5. Adjust the size of your system volume to meet the needs of your models.
6. Leave the default IP settings, ensuring that a public IPv4 address is assigned to the Instance.
7. Enter a name for your Instance to easily identify it, or use the default suggested name.
8. Verify your SSH key fingerprint is shown. If not, click **Add SSH key** and paste the SSH key you have [pre-generated](/identity-and-access-management/organizations-and-projects/how-to/create-ssh-key/) for secure access to the Instance.
9. Review the cost estimation, and click **Create Instance** to proceed. The Instance may take a few minutes to start. Once it is up and running, copy its public IP address.

## Installing an Ampere-optimized AI framework

1. Use SSH to access the system by specifying the user as `root` followed by the public IP address of your Instance.
    ```bash
    ssh root@<your_instance_ip>
    ```
2. Execute the following command to update the package lists and install Docker once logged in to the Instance.
    ```bash
    apt update && apt install -y docker.io
    ```
    <Message type="tip">
      While the Docker installation completes, visit the [Ampere AI DockerHub](https://hub.docker.com/u/amperecomputingai) and select one of the three available Ampere-optimized frameworks: PyTorch, ONNX Runtime, or TensorFlow.
      In this tutorial we use the PyTorch image, you can adapt it towards your requirements.
    </Message>

3. Run the Ampere-optimized PyTorch image. To do so, execute the following command in your Instance's shell:
    ```bash
    docker run --privileged=true --name ampere-torch --network host -it amperecomputingai/pytorch:latest
    ```
    The selected image will be automatically pulled, and a Docker container will be initialized. You should see the Ampere AI ASCII art indicating successful initialization.
    <Lightbox src="scaleway-ampere-ai-pytorch.webp" alt="" size="large" />
4. Import your AI library of choice in Python to verify the success of the installation.
    ```bash
    python3 -c "import torch"
    ```

## Conclusion

By following this tutorial, you have successfully deployed an Ampere-optimized framework on your COP-ARM Instance. By running your AI workloads on COP-ARM Instances, you can leverage the power of ARM architecture for your AI workloads, potentially achieving better performance and efficiency. Go further by fine-tuning your deployment with different models and configurations to optimize your setup further.