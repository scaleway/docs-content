---
meta:
  title: Managed Inference model catalog
  description: Deploy your own model with Scaleway Managed Inference. Privacy-focused, fully managed.
content:
  h1:  Managed Inference model catalog
  paragraph: This page provides information on the Scaleway Managed Inference product catalog
tags:
dates:
  validation: 2025-03-19
  posted: 2024-05-28
categories:
  - ai-data
---

A quick overview of available models in Scaleway's catalog and their core attributes. Expand any model below to see usage examples, curl commands, and detailed capabilities.

## Summary table

| Model Name | Provider | Context Size | Modalities | Instances | License |
|------------|----------|--------------|------------|-----------|---------|
| `mixtral-8x7b-instruct-v0.1` | Mistral | 32k tokens | Text | H100 | Apache 2.0 |
| `llama-3.1-70b-instruct` | Meta | 32k tokens | Text | H100, H100-2 | Llama 3 community |
| `llama-3.1-8b-instruct` | Meta | up to 128k tokens | Text | L4, L40S, H100, H100-2 | Llama 3 community |
| `llama-3-70b-instruct` | Meta | 8k tokens | Text | H100 | Llama 3 community |
| `llama-3.3-70b-instruct` | Meta | up to 131k tokens | Text | H100, H100-2 | Llama 3 community |
| `llama-3-nemotron-70b` | Nvidia | up to 128k tokens | Text | H100, H100-2 |Lllama 3.3 community |
| `deepseek-r1-distill-70b` | Deepseek | up to 131k tokens | Text | H100, H100-2 | MIT |
| `deepseek-r1-distill-8b` | Deepseek | up to 131k tokens | Text | L4, L40S, H100 | Apache 2.0 |
| `mistral-7b-instruct-v0.3` | Mistral | 32k tokens | Text | L4, L40S, H100, H100-1 | Apache 2.0 |
| `mistral-small-24b-instruct-2501` | Mistral | 32k tokens | Text | L40S, H100, H100-2 | Apache 2.0 |
| `mistral-nemo-instruct-2407` | Mistral | 128k | Text | L40S, H100, H100-2 | Apache 2.0 |
| `moshiko-0.1-8b` | Kyutai | 4,096 tokens | Text | L4, H100 | Apache 2.0 |
| `moshika-0.1-8b` | Kyutai | 4,096 tokens | Text | L4, H100 | Apache 2.0 |
| `wizardlm-70b-v1.0` | WizardLM | 4,096 tokens | Text | H100, H100-2 | Lllama 2 community |
| `pixtral-12b-2409` | Mistral | 128k tokens | Multimodal | L40S, H100, H100-2 | Apache 2.0 |
| `molmo-72b-0924` | Allen AI | 50k | Multimodal | H100-2 | Apache 2.0 |
| `qwen2.5-coder-32b-instruct` | Qwen | up to 32k | Code | H100, H100-2 | Qianwen License |
| `sentence-t5-xxl` | Sentence transformers | 512 tokens | Embeddings | L4 | Apache 2.0 |

## Model details

<Message type="note">
  Despite efforts for accuracy, the possibility of generated text containing inaccuracies or [hallucinations](/managed-inference/concepts/#hallucinations) exists. Always verify the content generated independently.
</Message>

## Text models

<Concept>

### Mixtral-8x7b-instruct-v0.1

  Mixtral-8x7b-instruct-v0.1, developed by Mistral, is tailored for instructional platforms and virtual assistants.
  Trained on vast instructional datasets, it provides clear and concise instructions across various domains, enhancing user learning experiences.

  | Attribute             | Details |
  |----------------------|---------|
  | Provider             | Mistral |
  | Context Size         | 32k tokens |
  | License              | Apache 2.0 |
  | Compatible Instances | H100 |

  #### Model names

  ```bash
  mistral/mixtral-8x7b-instruct-v0.1:fp8
  mistral/mixtral-8x7b-instruct-v0.1:bf16
  ```
  #### Sending Inference requests

  To perform inference tasks with your Mixtral model deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"mistral/mixtral-8x7b-instruct-v0.1:fp8", "messages":[{"role": "user","content": "Sing me a song about Scaleway"}], "max_tokens": 200, "top_p": 1, "temperature": 1, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

</Concept>

<Concept>
  ### LLaMA 3.1 70B Instruct

  Released July 23, 2024, Meta’s Llama 3.1 is an iteration of the open-access Llama family.
  Llama 3.1 was designed to match the best proprietary models, outperform many of the available open source on common industry benchmarks.

  | Attribute             | Details |
  |----------------------|---------|
  | Provider             | Meta |
  | Context Size         | 32k tokens |
  | License              | Llama 3 community |
  | Compatible Instances | H100 17k (FP8), H100-2 128k (FP8), 70k (BF16) |


  #### Sending Managed Inference requests

  To perform inference tasks with your Llama-3.1 deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"meta/llama-3.1-70b-instruct:fp8", "messages":[{"role": "user","content": "There is a llama in my garden, what should I do?"}], "max_tokens": 500, "temperature": 0.7, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

</Concept>

<Concept>

### Llama-3.1-8b-instruct model

  Released July 23, 2024, Meta’s Llama 3.1 is an iteration of the open-access Llama family.
  Llama 3.1 was designed to match the best proprietary models, outperform many of the available open source on common industry benchmarks.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Meta](https://llama.meta.com/llama3/)  |
  | License        | [Llama 3.1 community](https://llama.meta.com/llama3_1/license/)  |
  | Compatible Instances | L4, L40S, H100, H100-2 (FP8, BF16) |
  | Context Length | up to 128k tokens |

  #### Model names

  ```bash
  meta/llama-3.1-8b-instruct:fp8
  meta/llama-3.1-8b-instruct:bf16
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | L4      | 96k (FP8), 27k (BF16) | 
  | L40S    | 128k (FP8, BF16) | 
  | H100      | 128k (FP8, BF16) |
  | H100-2      | 128k (FP8, BF16) |

  #### Sending Managed Inference requests

  To perform inference tasks with your Llama-3.1 deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"meta/llama-3.1-8b-instruct:fp8", "messages":[{"role": "user","content": "There is a llama in my garden, what should I do?"}], "max_tokens": 500, "temperature": 0.7, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

</Concept>

<Concept>

### Llama-3-70b-instruct

  Meta’s Llama 3 is an iteration of the open-access Llama family.
  Llama 3 was designed to match the best proprietary models, enhanced by community feedback for greater utility and responsibly spearheading the deployment of LLMs.
  With a commitment to open-source principles, this release marks the beginning of a multilingual, multimodal future for Llama 3, pushing the boundaries in reasoning and coding capabilities.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Meta](https://llama.meta.com/llama3/)  |
  | Compatible Instances | H100, H100-2 (FP8)    |
  | Context size | 8192 tokens   |

  #### Model names

  ```bash
  meta/llama-3-70b-instruct:fp8
  ```

  #### Compatible Instances

  - [H100 (FP8)](https://www.scaleway.com/en/h100-pcie-try-it-now/)
  - H100-2 (FP8)

  #### Sending Managed Inference requests

  To perform inference tasks with your Llama-3 deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"meta/llama-3-70b-instruct:fp8", "messages":[{"role": "user","content": "Sing me a song about Xavier Niel"}], "max_tokens": 500, "top_p": 1, "temperature": 0.7, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

</Concept>

<Concept>

### Llama-3.3-70b-instruct

  Released December 6, 2024, Meta’s Llama 3.3 70b is a fine-tune of the [Llama 3.1 70b](/managed-inference/reference-content/llama-3.1-70b-instruct/) model.
  This model is still text-only (text in/text out). However, Llama 3.3 was designed to approach the performance of Llama 3.1 405B on some applications.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Meta](https://www.llama.com/)  |
  | License        | [Llama 3.3 community](https://www.llama.com/llama3_3/license/)  |
  | Compatible Instances | H100 (FP8), H100-2 (FP8, BF16) |
  | Context length | Up to 131k tokens    |

  #### Model names

  ```bash
  meta/llama-3.3-70b-instruct:bf16
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | H100      | 15k (FP8) |
  | H100-2      | 131k (FP8), 62k (BF16) |

  #### Sending Managed Inference requests

  To perform inference tasks with your Llama-3.3 deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"meta/llama-3.3-70b-instruct:bf16", "messages":[{"role": "user","content": "There is a llama in my garden, what should I do?"}], "max_tokens": 500, "temperature": 0.7, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

</Concept>

<Concept>

### Llama-3.1-Nemotron-70b-instruct

  Introduced October 14, 2024, NVIDIA's Nemotron 70B Instruct is a specialized version of the Llama 3.1 model designed to follow complex instructions. 
  NVIDIA employed Reinforcement Learning from Human Feedback (RLHF) to fine-tune the model’s ability to generate relevant and informative responses.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Nvidia](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct)  |
  | License        | [Llama 3.1 community](https://llama.meta.com/llama3_1/license/)  |                |
  | Compatible Instances | H100 (FP8), H100-2 (FP8) |
  | Context Length | up to 128k tokens    |

  #### Model names

  ```bash
  meta/llama-3.1-nemotron-70b-instruct:fp8
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | H100      | 16k (FP8) |
  | H100-2      | 128k (FP8) |


  #### Sending Managed Inference requests

  To perform inference tasks with your Llama-3.1-Nemotron-70b-instruct deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"meta/llama-3.1-nemotron-70b-instruct:fp8", "messages":[{"role": "user","content": "There is a llama in my garden, what should I do?"}], "max_tokens": 500, "temperature": 0.7, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

</Concept>

<Concept>

### DeepSeek-R1-Distill-Llama-70B

  Released January 21, 2025, Deepseek’s R1 Distilled Llama 70B is a distilled version of the Llama model family based on Deepseek R1.
  DeepSeek R1 Distill Llama 70B is designed to improve the performance of Llama models on reasoning use case such as mathematics and coding tasks.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Deepseek](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)  |
  | License        | [MIT](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md)  |
  | Compatible Instances | H100 (FP8), H100-2 (FP8, BF16) |
  | Context Length | up to 131k tokens |

  #### Model names

  ```bash
  deepseek/deepseek-r1-distill-llama-70b:bf16
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | H100      | 15k (FP8) |
  | H100-2      | 131k (FP8), 56k (BF16) |

  #### Sending Managed Inference requests

  To perform inference tasks with your DeepSeek R1 Distill Llama deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"deepseek/deepseek-r1-distill-llama-70b:fp8", "messages":[{"role": "user","content": "There is a llama in my garden, what should I do?"}], "max_tokens": 500, "temperature": 0.7, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (user, assistant) and content.
  </Message>

  <Message type="tip">
    This model is better used without `system prompt`, as suggested by the model provider.
  </Message>

</Concept>

<Concept>

### DeepSeek-R1-Distill-Llama-8B

  Released January 21, 2025, Deepseek’s R1 Distilled Llama 8B is a distilled version of the Llama model family based on Deepseek R1.
  DeepSeek R1 Distill Llama 8B is designed to improve the performance of Llama models on reasoning use cases such as mathematics and coding tasks.


  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Deepseek](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)  |
  | License        | [MIT](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md)  |
  | Compatible Instances | L4, L40S, H100 (FP8, BF16) |
  | Context Length | up to 131k tokens |

  #### Model names

  ```bash
  deepseek/deepseek-r1-distill-llama-8b:bf16
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | L4      | 90k (FP8), 39k (BF16) | 
  | L40S      | 131k (FP8, BF16) | 
  | H100      | 131k (FP8, BF16) |

  #### Sending Managed Inference requests

  To perform inference tasks with your DeepSeek R1 Distill Llama deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"deepseek/deepseek-r1-distill-llama-8b:fp8", "messages":[{"role": "user","content": "There is a llama in my garden, what should I do?"}], "max_tokens": 500, "temperature": 0.7, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (user, assistant) and content.
  </Message>

  <Message type="tip">
    This model is better used without `system prompt`, as suggested by the model provider.
  </Message>

</Concept>

<Concept>

### Mistral-7b-instruct-v0.3

  The first dense model released by Mistral AI, perfect for experimentation, customization, and quick iteration. At the time of the release, it matched the capabilities of models up to 30B parameters.
  This model is open-weight and distributed under the Apache 2.0 license.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Mistral](https://mistral.ai/technology/#models)     |
  | Compatible Instances | L4, L40S, H100, H100-2 (BF16)     |
  | Context size | 32K tokens    |

  #### Model name

  ```bash
  mistral/mistral-7b-instruct-v0.3:bf16
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | L4      | 32k (BF16) |
  | L40S      | 32k (BF16) |
  | H100      | 32k (BF16) |
  | H100-2      | 32k (BF16) |

  #### Sending Inference requests

  To perform inference tasks with your Mistral model deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"mistral/mistral-7b-instruct-v0.3:bf16", "messages":[{"role": "user","content": "Explain Public Cloud in a nutshell."}], "top_p": 1, "temperature": 0.7, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

</Concept>

<Concept>

###  Mistral-small-24b-base-2501

  Mistral Small 24B Instruct is a state-of-the-art transformer model of 24B parameters, built by Mistral.
  This model is open-weight and distributed under the Apache 2.0 license.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Mistral](https://mistral.ai/technology/#models)  |
  | Compatible Instances | L40S, H100, H100-2 (FP8) |
  | Context size | 32K tokens |

  #### Model name

  ```bash
  mistral/mistral-small-24b-instruct-2501:fp8
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | L40      | 20k (FP8) |
  | H100      | 32k (FP8) |
  | H100-2      | 32k (FP8) |

  #### Sending Inference requests

  To perform inference tasks with your Mistral model deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"mistral/mistral-small-24b-instruct-2501:fp8", "messages":[{"role": "user","content": "Tell me about Scaleway."}], "top_p": 1, "temperature": 0.7, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

</Concept>

<Concept>

### Mistral-nemo-instruct-2407

  Mistral Nemo is a state-of-the-art transformer model of 12B parameters, built by Mistral in collaboration with NVIDIA.
  This model is open-weight and distributed under the Apache 2.0 license.
  It was trained on a large proportion of multilingual and code data.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Mistral](https://mistral.ai/technology/#models)  |
  | Compatible Instances | L40S, H100, H100-2 (FP8) |
  | Context size | 128K tokens |

  #### Model name

  ```bash
  mistral/mistral-nemo-instruct-2407:fp8
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | L40      | 128k (FP8) |
  | H100      | 128k (FP8) |
  | H100-2      | 128k (FP8) |

  #### Sending Inference requests

  <Message type="tip">
    Unlike previous Mistral models, Mistral Nemo requires smaller temperatures. It is recommend to use a temperature of 0.35.
  </Message>

  To perform inference tasks with your Mistral model deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"mistral/mistral-nemo-instruct-2407:fp8", "messages":[{"role": "user","content": "Sing me a song about Xavier Niel"}], "top_p": 1, "temperature": 0.35, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

</Concept>

<Concept>

### Moshiko-0.1-8b

  Kyutai's Moshi is a speech-text foundation model for real-time dialogue.
  Moshi is an experimental next-generation conversational model, designed to understand and respond fluidly and naturally to complex conversations, while providing unprecedented expressiveness and spontaneity.
  While current systems for spoken dialogue rely on a pipeline of separate components, Moshi is the first real-time full-duplex spoken large language model.
  Moshiko is the variant of Moshi with a male voice in English.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Kyutai](https://github.com/kyutai-labs/moshi)  |
  | Compatible Instances | L4, H100 (FP8, BF16)    |
  | Context size | 4096 tokens   |

  #### Model names

  ```bash
  kyutai/moshiko-0.1-8b:bf16
  kyutai/moshiko-0.1-8b:fp8
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | L4      | 4096 (FP8, BF16) | 
  | H100      | 4096 (FP8, BF16) |

  #### How to use it

  To perform inference tasks with your Moshi deployed at Scaleway, a WebSocket API is exposed for real-time dialogue and is accessible at the following endpoint:

  ```bash
  wss://<Deployment UUID>.ifr.fr-par.scaleway.com/api/chat
  ```

  #### Testing the WebSocket endpoint

  To test the endpoint, use the following command:

  ```bash
  curl -i --http1.1 \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Connection: Upgrade" \
  -H "Upgrade: websocket" \
  -H "Sec-WebSocket-Key: SGVsbG8sIHdvcmxkIQ==" \
  -H "Sec-WebSocket-Version: 13" \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/api/chat"
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    Authentication can be done using the `token` query parameter, which should be set to your IAM API key, if headers are not supported (e.g., in a browser).
  </Message>

  The server should respond with a `101 Switching Protocols` status code, indicating that the connection has been successfully upgraded to a WebSocket connection.

  #### Interacting with the model

  We provide code samples in various programming languages (Python, Rust, typescript) to interact with the model using the WebSocket API as well as a simple web interface.
  Those code samples can be found in our [GitHub repository](https://github.com/scaleway/moshi-client-examples).
  This repository contains instructions on how to run the code samples and interact with the model.

</Concept>

<Concept>

### Moshika-0.1-8b


  Kyutai's Moshi is a speech-text foundation model for real-time dialogue.
  Moshi is an experimental next-generation conversational model, designed to understand and respond fluidly and naturally to complex conversations, while providing unprecedented expressiveness and spontaneity.
  While current systems for spoken dialogue rely on a pipeline of separate components, Moshi is the first real-time full-duplex spoken large language model.
  Moshiko is the variant of Moshi with a male voice in English.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Kyutai](https://github.com/kyutai-labs/moshi)  |
  | Compatible Instances | L4, H100 (FP8, BF16)    |
  | Context size | 4096 tokens   |

  #### Model names

  ```bash
  kyutai/moshiko-0.1-8b:bf16
  kyutai/moshiko-0.1-8b:fp8
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | L4      | 4096 (FP8, BF16) | 
  | H100      | 4096 (FP8, BF16) |

  #### How to use it

  To perform inference tasks with your Moshi deployed at Scaleway, a WebSocket API is exposed for real-time dialogue and is accessible at the following endpoint:

  ```bash
  wss://<Deployment UUID>.ifr.fr-par.scaleway.com/api/chat
  ```

  #### Testing the WebSocket endpoint

  To test the endpoint, use the following command:

  ```bash
  curl -i --http1.1 \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Connection: Upgrade" \
  -H "Upgrade: websocket" \
  -H "Sec-WebSocket-Key: SGVsbG8sIHdvcmxkIQ==" \
  -H "Sec-WebSocket-Version: 13" \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/api/chat"
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    Authentication can be done using the `token` query parameter, which should be set to your IAM API key, if headers are not supported (e.g., in a browser).
  </Message>

  The server should respond with a `101 Switching Protocols` status code, indicating that the connection has been successfully upgraded to a WebSocket connection.

  #### Interacting with the model

  We provide code samples in various programming languages (Python, Rust, typescript) to interact with the model using the WebSocket API as well as a simple web interface.
  Those code samples can be found in our [GitHub repository](https://github.com/scaleway/moshi-client-examples).
  This repository contains instructions on how to run the code samples and interact with the model.

</Concept>

<Concept>

### WizardLM-70B-V1.0

  WizardLM-70B-V1.0, developed by WizardLM, is specifically designed for content creation platforms and writing assistants.
  With its extensive training in diverse textual data, WizardLM-70B-V1.0 generates high-quality content and assists writers in various creative and professional endeavors.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [WizardLM](https://wizardlm.github.io/WizardLM2/) |
  | Compatible Instances | H100 (FP8) - H100-2 (FP16)    |
  | Context size | 4,096 tokens    |

  #### Model names

  ```bash
  wizardlm/wizardlm-70b-v1.0:fp8
  wizardlm/wizardlm-70b-v1.0:fp16
  ```

  #### Compatible Instances

  - [H100-1 (INT8)](https://www.scaleway.com/en/h100-pcie-try-it-now/)
  - [H100-2 (FP16)](https://www.scaleway.com/en/h100-pcie-try-it-now/)

  #### Sending Inference requests

  To perform inference tasks with your WizardLM model deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"wizardlm/wizardlm-70b-v1.0:fp8", "messages":[{"role": "user","content": "Say hello to Scaleway's Inference"}], "max_tokens": 200, "top_p": 1, "temperature": 1, "stream": false}'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

</Concept>

## Multimodal models

<Concept>

### Pixtral-12b-2409

  Pixtral is a vision language model introducing a novel architecture: 12B parameter multimodal decoder plus 400M parameter vision encoder.
  It can analyze images and offer insights from visual content alongside text.
  This multimodal functionality creates new opportunities for applications that need both visual and textual comprehension.

  Pixtral is open-weight and distributed under the Apache 2.0 license.

  <Message type="note">
    Pixtral 12B can understand and analyze images, not generate them. You will use it through the /v1/chat/completions endpoint.
  </Message>


  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Mistral](https://mistral.ai/technology/#models)    |
  | Compatible Instances | L40S, H100, H100-2 (bf16)      |
  | Context size | 128k tokens  |

  #### Model name

  ```bash
  mistral/pixtral-12b-2409:bf16
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | L40S      | 50k (BF16)
  | H100      | 128k (BF16)
  | H100-2      | 128k (BF16)

  #### Sending Inference requests

  <Message type="tip">
    Unlike previous Mistral models, Pixtral can take an `image_url` in the content array.
  </Message>

  To perform inference tasks with your Pixtral model deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scw.cloud/v1/chat/completions" \
  --data '{
        "model": "mistral/pixtral-12b-2409:bf16",
        "messages": [
          {
            "role": "user",
            "content": [
                {"type" : "text", "text": "Describe this image in detail please."},
                {"type": "image_url", "image_url": {"url": "https://picsum.photos/id/32/512/512"}},
                {"type" : "text", "text": "and this one as well."},
                {"type": "image_url", "image_url": {"url": "https://www.wolframcloud.com/obj/resourcesystem/images/a0e/a0ee3983-46c6-4c92-b85d-059044639928/6af8cfb971db031b.png"}}
            ]
          }
        ],
        "top_p": 1,
        "temperature": 0.7,
        "stream": false
  }'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

  #### Passing images to Pixtral

  1. Image URLs
  If the image is available online, you can just include the image URL in your request as demonstrated above. This approach is simple and does not require any encoding.

  2. Base64 encoded image
  Base64 encoding is a standard way to transform binary data, like images, into a text format, making it easier to transmit over the internet.

  The following Python code sample shows you how to encode an image in base64 format and pass it to your request payload.


  ```python
  import base64
  from io import BytesIO
  from PIL import Image

  def encode_image(img):
      buffered = BytesIO()
      img.save(buffered, format="JPEG")
      encoded_string = base64.b64encode(buffered.getvalue()).decode("utf-8")
      return encoded_string

  img = Image.open("path_to_your_image.jpg")
  base64_img = encode_image(img)

  payload = {
      "messages": [
          {
              "role": "user",
              "content": [
                  {"type": "text", "text": "What is this image?"},
                  {
                      "type": "image_url",
                      "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"},
                  },
              ],
          }
      ],
      ... # other parameters
  }

  ```

  #### Receiving Managed Inference responses

  Upon sending the HTTP request to the public or private endpoints exposed by the server, you will receive inference responses from the managed Managed Inference server. 
  Process the output data according to your application's needs. The response will contain the output generated by the visual language model based on the input provided in the request.

  <Message type="note">
    Despite efforts for accuracy, the possibility of generated text containing inaccuracies or [hallucinations](/managed-inference/concepts/#hallucinations) exists. Always verify the content generated independently.
  </Message>

  #### Frequently Asked Questions

  ##### What types of images are supported by Pixtral?
  - Bitmap (or raster) image formats, meaning storing images as grids of individual pixels, are supported: PNG, JPEG, WEBP, and non-animated GIFs in particular.
  - Vector image formats (SVG, PSD) are not supported.

  ##### Are other files supported?
  Only bitmaps can be analyzed by Pixtral, PDFs and videos are not supported.

  ##### Is there a limit to the size of each image?
  Images size are limited:
  - Directly by the maximum context window. As an example, since tokens are squares of 16x16 pixels, the maximum context window taken by a single image is `4096` tokens (ie. `(1024*1024)/(16*16)`)
  - Indirectly by the model accuracy: resolution above 1024x1024 will not increase model output accuracy. Indeed, images above 1024 pixels width or height will be automatically downscaled to fit within 1024x1024 dimension. Note that image ratio and overall aspect is preserved (images are not cropped, only additionaly compressed).

  ##### What is the maximum amount of images per conversation?
  One conversation can handle up to 12 images (per request). The 13rd will return a 413 error.

</Concept>

<Concept>

### Molmo-72b-0924

  Molmo 72B is the powerhouse of the Molmo family, multimodal models developed by the renowned research lab Allen Institute for AI.
  Vision-language models like Molmo can analyze an image and offer insights from visual content alongside text. This multimodal functionality creates new opportunities for applications that need both visual and textual comprehension. 

  Molmo is open-weight and distributed under the Apache 2.0 license. All artifacts (code, data set, evaluations) are also expected to be fully open-source.
  Its base model is Qwen2-72B ([Twonyi Qianwen license](https://huggingface.co/Qwen/Qwen2-72B/blob/main/LICENSE)).

  <Message type="note">
    Molmo-72b can understand and analyze images, not generate them. You will use it through the /v1/chat/completions endpoint.
  </Message>

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Allen Institute for AI](https://molmo.allenai.org/blog)                         |
  | License        | Apache 2.0  |                |
  | Compatible Instances | H100-2 (FP8)                 |
  | Context size | 50k tokens    |

  #### Model name

  ```bash
  allenai/molmo-72b-0924:fp8
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | H100-2      | 50k (FP8)

  #### Sending inference requests

  <Message type="tip">
    Unlike regular chat models, Molmo-72b can take an `image_url` in the content array.
  </Message>

  To perform inference tasks with your Molmo-72b model deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scw.cloud/v1/chat/completions" \
  --data '{
        "model": "allenai/molmo-72b-0924:fp8",
        "messages": [
          {
            "role": "user",
            "content": [
                {"type" : "text", "text": "Describe this image in detail please."},
                {"type": "image_url", "image_url": {"url": "https://picsum.photos/id/32/512/512"}}
            ]
          }
        ],
        "top_p": 1, 
        "temperature": 0.7, 
        "stream": false
  }'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  #### Passing images to Molmo-72b

  ##### Image URLs
  If the image is available online, you can just include the image URL in your request as demonstrated above. This approach is simple and does not require any encoding.

  ##### Base64 encoded image
  Base64 encoding is a standard way to transform binary data, like images, into a text format, making it easier to transmit over the internet.

  The following Python code sample shows you how to encode an image in base64 format and pass it to your request payload.

  ```python
  import base64
  from io import BytesIO
  from PIL import Image

  def encode_image(img):
      buffered = BytesIO()
      img.save(buffered, format="JPEG")
      encoded_string = base64.b64encode(buffered.getvalue()).decode("utf-8")
      return encoded_string

  img = Image.open("path_to_your_image.jpg")
  base64_img = encode_image(img)

  payload = {
      "messages": [
          {
              "role": "user",
              "content": [
                  {"type": "text", "text": "What is this image?"},
                  {
                      "type": "image_url",
                      "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"},
                  },
              ],
          }
      ],
      ... # other parameters
  }

  ```

  #### Frequently Asked Questions

  ##### What types of images are supported by Molmo-72b?
  - Bitmap (or raster) image formats, meaning storing images as grids of individual pixels, are supported: PNG, JPEG, WEBP, and non-animated GIFs in particular.
  - Vector image formats (SVG, PSD) are not supported.

  ##### Are other file types supported?
  Only bitmaps can be analyzed by Molmo. PDFs and videos are not supported.

  ##### Is there a limit to the size of each image?
  The only limitation is the context window (1 token for each 16x16 pixel).

  ##### What is the maximum amount of images per conversation?
  One conversation can handle a maximum of 1 image (per request). Sending more than one image will return a 400 error.

</Concept>

## Code models

<Concept>

### Qwen2.5-coder-32b-instruct

  Qwen2.5-coder is your intelligent programming assistant familiar with more than 40 programming languages.
  With Qwen2.5-coder deployed at Scaleway, your company can benefit from code generation, AI-assisted code repair, and code reasoning. 

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [Qwen](https://qwenlm.github.io/)  |
  | License        | [Apache 2.0](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct/blob/main/LICENSE)  |
  | Compatible Instances | H100, H100-2 (INT8) |
  | Context Length | up to 32k tokens |

  #### Model names

  ```bash
  qwen/qwen2.5-coder-32b-instruct:int8
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | H100      | 32k (INT8)
  | H100-2      | 32k (INT8)

  #### Sending Managed Inference requests

  To perform inference tasks with your Qwen2.5-coder deployed at Scaleway, use the following command:

  ```bash
  curl -s \
  -H "Authorization: Bearer <IAM API key>" \
  -H "Content-Type: application/json" \
  --request POST \
  --url "https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/chat/completions" \
  --data '{"model":"qwen/qwen2.5-coder-32b-instruct:int8", "messages":[{"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful code assistant."},{"role": "user","content": "Write a quick sort algorithm."}], "max_tokens": 1000, "temperature": 0.8, "stream": false}'
  ```

  <Message type="tip">
    The model name allows Scaleway to put your prompts in the expected format.
  </Message>

  <Message type="note">
    Ensure that the `messages` array is properly formatted with roles (system, user, assistant) and content.
  </Message>

</Concept>

## Embeddings models

<Concept>

### Sentence-t5-xxl

  The Sentence-T5-XXL model represents a significant evolution in sentence embeddings, building on the robust foundation of the Text-To-Text Transfer Transformer (T5) architecture.
  Designed for performance in various language processing tasks, Sentence-T5-XXL leverages the strengths of T5's encoder-decoder structure to generate high-dimensional vectors that encapsulate rich semantic information.
  This model has been meticulously tuned for tasks such as text classification, semantic similarity, and clustering, making it a useful tool in the RAG (Retrieval-Augmented Generation) framework. It excels in sentence similarity tasks, but its performance in semantic search tasks is less optimal.

  | Attribute       | Details                            |
  |-----------------|------------------------------------|
  | Provider        | [sentence-transformers](https://www.sbert.net/)  |
  | Compatible Instances | L4 (FP32)    |
  | Context size | 512 tokens    |

  #### Model name

  ```bash
  sentence-transformers/sentence-t5-xxl:fp32
  ```

  #### Compatible Instances

  | Instance type  | Max context length |
  | ------------- |-------------|
  | L4      | 512 (FP32) |

  #### Sending Managed Inference requests

  To perform inference tasks with your Embedding model deployed at Scaleway, use the following command:

  ```bash
  curl https://<Deployment UUID>.ifr.fr-par.scaleway.com/v1/embeddings \
    -H "Authorization: Bearer <IAM API key>" \
    -H "Content-Type: application/json" \
    -d '{
      "input": "Embeddings can represent text in a numerical format.",
      "model": "sentence-transformers/sentence-t5-xxl:fp32"
    }'
  ```

  Make sure to replace `<IAM API key>` and `<Deployment UUID>` with your actual [IAM API key](/iam/how-to/create-api-keys/) and the Deployment UUID you are targeting.

</Concept>