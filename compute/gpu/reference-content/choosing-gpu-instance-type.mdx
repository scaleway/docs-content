---
meta:
  title: Choosing the right GPU Instance type
  description: This page provides information about how to choose a GPU Instance type
content:
  h1: Choosing the right GPU Instance type
  paragraph: This section provides information about how to choose a GPU Instance type
tags: NVIDIA GPU cloud instance
dates:
  validation: 2023-08-31
  posted: 2022-08-31
categories:
  - compute
---

A GPU Instance refers to a virtual computing environment provided by Scaleway that offers access to powerful Graphics Processing Units (GPUs) over the internet.
GPUs are specialized hardware originally designed for rendering graphics in video games and other 3D applications.
However, their massively parallel architecture makes them ideal for various high-performance computing tasks, such as machine learning, data processing, scientific simulations, and more.

Scaleway GPU Instances' availability has revolutionized how researchers, developers, and organizations train complex machine-learning models faster and more efficiently.
It empowers European AI startups, giving them the means (without the need for a huge CAPEX investment) to create products that revolutionize how we work and live.

## How to choose the right GPU Instance type

Scaleway provides a range of GPU Instance offers. There are several factors to consider when choosing the right GPU Instance type, to ensure that it meets your performance, budget, and scalability requirements.
Below, you will find a guide to help you make an informed decision:

* **Workload requirements:** Identify the nature of your workload. Are you running machine learning, deep learning, high-performance computing (HPC), data analytics, or graphics-intensive applications? Different Instance types are optimized for different types of workloads. For example, the H100 is not designed for graphics rendering. However, the L4 and L40S are. As [stated by Tim Dettmers](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/),  “Tensor Cores are most important, followed by memory bandwidth of a GPU, the cache hierarchy, and only then FLOPS of a GPU.”. For more information, refer to the [NVIDIA GPU portfolio](https://www.nvidia.com/content/dam/en-zz/solutions/data-center/data-center-gpu-portfolio-line-card.pdf).
* **Performance requirements:** Evaluate the performance specifications you need, such as the number of GPUs, GPU memory, processing power, and network bandwidth. You need a lot of memory and fast storage for demanding tasks like deep learning.
* **GPU type:** Scaleway offers different GPU types, such as various NVIDIA GPUs. Each GPU has varying levels of performance, memory, and capabilities. Choose a GPU that aligns with your specific workload requirements.
* **GPU memory:** GPU memory bandwidth is an important criterion influencing overall performance. Larger GPU memory is crucial for memory-intensive tasks like deep learning, training on large datasets or GPU rendering. Modern GPUs offer specialized data formats designed to optimize deep learning performance. These formats, including Bfloat16, FP8, int8 and int4, enable the storage of more data in memory and can enhance performance by a factor of ten. To make an informed decision, it is thus crucial to select the appropriate NVIDIA architecture—options range from Pascal and Ampere to Ada Lovelace and Hopper. Ensuring that the GPU possesses sufficient memory capacity to accommodate your specific workload is essential, preventing any potential memory-related bottlenecks. Equally important, is matching the GPU's memory type to the nature of your workload. Keep in mind that HBM3 memory surpasses HBM2e which in turn, outperforms HBM2 and GDDR6, making the right memory choice a key consideration.
* **CPU and RAM:**  A powerful CPU can be beneficial for tasks that involve preprocessing or post-processing. Sufficient system memory is also crucial to prevent memory-related bottlenecks.
* **GPU driver and software compatibility:** Ensure that the GPU Instance type you choose supports the GPU drivers and software frameworks you need for your workload. This includes CUDA libraries, machine learning frameworks (TensorFlow, PyTorch, etc.), and other specific software tools. For all [Scaleway GPU OS images](/compute/gpu/reference-content/docker-images/), we offer a driver version that enables the use of all GPUs, from the oldest to the latest models. As is the NGC CLI, `nvidia-docker` is pre-installed, enabling containers to be used with CUDA, cuDNN, and the main deep learning frameworks.
* **Scaling:** Consider the scalability requirements of your workload. The most efficient way to scale up your workload is by using:
  * Bigger GPU
  * Up to 2 PCIe GPU (available soon)
  * A NVIDIA DGX server-like setup with 8x NVlink GPUs
  * A NVIDIA AI supercomputer for a larger setup for workload-intensive tasks
  * Another way to scale your workload is to use [Kubernetes and MIG](/compute/gpu/how-to/use-nvidia-mig-technology/): You can divide a single H100 GPU into as many as 7 MIG partitions. This means that instead of employing seven P100 GPUs to set up seven K8S pods, you could opt for a single H100 GPU with MIG to effectively deploy all seven K8S pods.
* **Online resources:** Check for online resources, forums, and community discussions related to the specific GPU type you are considering. This can provide insights into common issues, best practices, and optimizations.

Remember that there is no one-size-fits-all answer, and the right GPU Instance type will depend on your workload’s unique requirements. It is important that you regularly reassess your choice as your workload evolves. Depending on which type best fits your evolving tasks, you can easily migrate from one GPU Instance type to another.

## Scaleway GPU Instances types overview

|                                                                     | RENDER-S                                                                            | GPU-3070-S                              | H100-1-80G                                                                                                                                       | H100-2-80G                                                                                                                                       |
|---------------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|
| GPU Type                                                            | 1x [P100](https://www.nvidia.com/en-us/data-center/tesla-p100/)                     | 1x 3070                                 | 1x [H100](https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet)                                                       | 2x [H100](https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet)                                                       |
| Tensor Cores                                                        | N/A                                                                                 | N/A                                     | Yes                                                                                                                                              | Yes                                                                                                                                              |
| Performance in TFLOPS (FP16 acc 32 Tensor Cores - without sparsity) | (No Tensor Cores :  9,3 TFLOPS FP32)                                                | 40,6 TFLOPS                             | 1513 TFLOPS                                                                                                                                      | 2x 1513 TFLOPS                                                                                                                                   |
| VRAM                                                                | 16 GB HBM2 (Memory bandwidth: 732 GB/s)                                             | 8 GB GDDR6 (Memory bandwidth: 448 GB/s) | 80 GB HBM3 (Memory bandwidth: 2TB/s)                                                                                                             | 2x80 GB HBM3 (Memory bandwidth: 2TB/s)                                                                                                           |
| CPU Type                                                            | Intel Xeon Gold 6148 (2.4 GHz)                                                      | AMD EPYC™ 7413 (2.65 GHz)               | AMD EPYC™ 9334 (2.7GHz)                                                                                                                          | AMD EPYC™ 9334 (2.7GHz)                                                                                                                          |
| vCPUs                                                               | 10                                                                                  | 8                                       | 24                                                                                                                                               | 48                                                                                                                                               |
| RAM                                                                 | 42 GB DDR3                                                                          | 16 GB DDR4                              | 240 GB DDR5                                                                                                                                      | 480 GB DDR5                                                                                                                                      |
| Storage                                                             | Block/Local                                                                         | Block                                   | Block                                                                                                                                            | Block                                                                                                                                            |
| Scratch Storage                                                     | No                                                                                  | No                                      | Yes (1.9 TB NVMe)                                                                                                                                | Yes (3.8 TB NVMe)                                                                                                                                |
| Bandwidth                                                           | 1 Gbps                                                                              | 2.5 Gbps                                | 10 Gbps                                                                                                                                          | 20 Gbps                                                                                                                                          |
| Better used for                                                     | - Graphic Computer Vision<br />- General Deep Learning usage<br />- Video Encoding/Decoding (~4k) |                                         | - Large size model training<br />- Fine-tune LLMs/transformers models<br />- Generative AI<br />- Optimize GPU workflows & deployments in Kubernetes thanks to MIG | - Large size model training<br />- Fine-tune LLMs/transformers models<br />- Generative AI<br />- Optimize GPU workflows & deployments in Kubernetes thanks to MIG |
| Not made for                                                        | Large Model (especially LLM)                                                        |                                         | No Graphic or video encoding use cases                                                                                                          | No Graphic or video encoding use cases                                                                                                           |