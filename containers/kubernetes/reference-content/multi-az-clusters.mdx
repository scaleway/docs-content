---
meta:
  title: Ensuring resiliency with Multi-AZ Kubernetes clusters
  description: This page explains how to ensure resiliency with Multi-AZ Kubernetes clusters
content:
  h1: Ensuring resiliency with Multi-AZ Kubernetes clusters
  paragraph: This page explains how to ensure resiliency with Multi-AZ Kubernetes clusters
tags: kubernetes persistent volume persistent-volume
dates:
  validation: 2023-10-17
  posted: 2023-10-17
categories:
  - kubernetes
---

With the introduction of regional Private Networks, Kubernetes Kapsule clusters can now benefit from an extra layer of security for their worker nodes.

It is now also possible to deploy different pools within a cluster across different Availability Zones (AZ).

<Lightbox src="scaleway-k8s-multi-az-cluster.webp" alt="" size="large" />

## Benefits of a multi-AZ configuration

- With the multi-AZ option, you can implement a disaster recovery strategy that includes the replication of your workload across multiple AZ. This helps ensure data resiliency even in the unlikely event of an AZs becoming suddenly unavailable.

- Before the introduction of multi-AZ clusters, if you set up pools in PAR3, they were limited to that specific zone. For example, PAR3 doesn't support GPU nodes. So, if you needed GPU nodes, you had to start a new cluster in either PAR1 or PAR2 and reconfigure from the beginning. By using the multi-AZ feature, you can have pools in different AZs while keeping a consistent cluster configuration.

  The multi-AZ option enables, for example, the possibility of establishing a pool within PAR3 using PLAY2-NANO nodes, and simultaneously creating another pool in PAR2 equipped with GPU nodes, all within the same cluster. This approach is particularly useful if you are looking to incorporate GPU nodes in your cluster, but face availability constraints in your preferred AZ.

  Similarly, this flexibility extends to situations where certain node types are fully allocated. If, for example, GPUs are unavailable in PAR3, you can now add them in PAR1, all while operating within the same cluster.

## Requirements of a multi-AZ setup

- In order to benefit from multi-AZ clusters, you must have a created a cluster compatible with and [attached to a Private Network](/containers/kubernetes/how-to/create-cluster/).

  <Message type="note">
    If your cluster is not attached to Private Network, make sure it is compatible with regional Private Networks first. If it is not, you can follow the procedure to migrate your cluster [via the console](/secure-cluster-with-private-network/#how-can-i-migrate-my-existing-clusters-to-regional-private-networks), [the API](https://www.scaleway.com/en/developers/api/kubernetes/#path-clusters-migrate-an-existing-cluster-to-a-private-network-cluster), or [via Terraform](https://registry.terraform.io/providers/scaleway/scaleway/latest/docs/resources/k8s_cluster#private_network_id).
  </Message>

- You must also make sure that the nodes you select for your pool are available and in stock in your AZ of choice.

  <Message type="important">
    Some node types are not available with Kubernetes. Not all node types are available in all AZs. Even if a node type is available in a certain AZs, it might be out of stock. Make sure you check the availability in the Scaleway console before creating your pools.
  </Message>

## Recommendations

To create a solid environment for data resiliency, we recommend that you set up your cluster with:

  - at least **three nodes** and
  - distributed across a **minimum of two AZs**

### Affinity and anti-affinity set up

Your pods contain storage elements known as block volumes, which are linked to specific Availability Zones.

Without established inter-pod affinity or anti-affinity guidelines, there is no clear directive for which nodes your pods should or should not operate on. The implication is that a pod cannot restart in a new region if its current region experiences an outage.

This issue arises because a pod linked to a block volume in one zone, say PAR1, is unable to operate on a node in a different zone, like PAR2.

For example, a database pod in PAR1 relies on a block volume within the same zone. If an unexpected disruption occurs in PAR1, this database cannot resume operations in PAR2 due to its reliance on the PAR1 block volume.

Considering this, we suggest that you implement affinity rules, specifying the appropriate nodes for your pods to run. Alternatively, anti-affinity rules can be used to identify where your pods should not operate.

For detailed instructions on setting these affinity parameters, please visit the [official Kubernetes documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity).

## How to create a multi-AZ cluster

If you have taken into consideration the requirements and recommendations listed above, you can now create a multi-AZ cluster.

To do so, you must [create a new cluster](/containers/kubernetes/how-to/create-cluster), one compatible with regional Private Networks, and add two or more pools located in different AZs.

This can be done either via the Scaleway console, the CLI, or Terraform.

<Message type="note">
  If you use another method, refer to our [API Reference Documentation](https://www.scaleway.com/en/developers/api/kubernetes/#path-pools-create-a-new-pool-in-a-cluster).
</Message>

In the example below, we use the [Scaleway CLI](https://github.com/scaleway/scaleway-cli/blob/master/docs/commands/k8s.md#create-a-new-pool-in-a-cluster).

Run the following command twice. Replace `<cluster-id>`, `<AZ>` and `<instance-type>` with their corresponding values. Make sure you replace `<AZ>` with different AZs for each pool.

```
scw k8s pool create cluster-id=<cluster-id> zone=<AZ> size=1 node-type=<instance-type>
```

To see an example Terraform configuration, refer to the [How to create a multi-AZ cluster with Terraform](/tutorials/k8s-kapsule-multi-az/) tutorial page.



