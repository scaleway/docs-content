---
meta:
  title: How to deploy a model on Scaleway Managed Inference
  description: This page explains how to deploy a model on Scaleway Managed Inference
content:
  h1: How to deploy a model on Scaleway Managed Inference
  paragraph: This page explains how to deploy a model on Scaleway Managed Inference
tags: managed-inference ai-data creating dedicated
dates:
  validation: 2025-04-09
  posted: 2024-03-06
---

<Macro id="requirements" />

- A Scaleway account logged into the [console](https://console.scaleway.com)
- [Owner](/iam/concepts/#owner) status or [IAM permissions](/iam/concepts/#permission) allowing you to perform actions in the intended Organization

1. Click the **AI** section of the [Scaleway console](https://console.scaleway.com/), and select **Managed Inference** from the side menu to access the Managed Inference dashboard.
2. Click **Deploy a model** to launch the model deployment wizard.
3. Provide the necessary information:
    - Select the desired model and quantization to use for your deployment [from the available options](/managed-inference/reference-content/).
        <Message type="important">
          Scaleway Managed Inference allows you to deploy various AI models, either from the Scaleway catalog or by importing a custom model. For detailed information about supported models, visit our [Supported models in Managed Inference](/managed-inference/reference-content/supported-models/) documentation.
        </Message>
        <Message type="note">
          Some models may require acceptance of an end-user license agreement. If prompted, review the terms and conditions and accept the license accordingly.
        </Message>
    - Choose the geographical **region** for the deployment.
    - For custom models: Choose the model quantization.
      <Message type="tip">
        Each model comes with a default quantization. Select lower bits quantization to improve performance and enable model to run on smaller GPU Nodes, while potentially reducing precision.
      </Message>
    - Specify the GPU Instance type to be used with your deployment.
4. Enter a **name** for the deployment, and optional tags.
5. Configure the **network connectivity** settings for the deployment:
    - Attach to a **Private Network** for secure communication and restricted availability. Choose an existing Private Network from the drop-down list, or create a new one.
    - Set up **Public connectivity** to access resources via the public internet. Authentication by API key is enabled by default.
    <Message type="important">
      - Enabling both private and public connectivity will result in two distinct endpoints (public and private) for your deployment.
      - Deployments must have at least one endpoint, either public or private.
    </Message>
6. Click **Deploy model** to launch the deployment process. Once the model is ready, it will be listed among your deployments.