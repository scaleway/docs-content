---
meta:
  title: Distributed Data Lab - Quickstart
  description: Get started with Scaleway Distributed Data Lab quickly and efficiently.
content:
  h1: Distributed Data Lab - Quickstart
  paragraph: Get started with Scaleway Distributed Data Lab quickly and efficiently.
tags: distributed data lab apache spark notebook jupyter processing
dates:
  validation: 2024-07-31
  posted: 2024-07-10
categories:
  - managed-services
  - data-lab
---

Distributed Data Lab is a product designed to assist data scientists and data engineers in performing calculations on a remotely managed Apache Spark infrastructure.

It is composed of the following:

  - Cluster: An Apache Spark cluster powered by a Kubernetes architecture.

  - Notebook: A JupyterLab service operating on a dedicated node type.

Scaleway provides dedicated node types for both the notebook and the cluster. The cluster nodes are high-end machines built for intensive computations, featuring numerous CPUs and substantial RAM. 

The notebook, although capable of performing some local computations, primarily serves as a web interface for interacting with the Apache Spark cluster.

<Macro id="requirements" />

- A Scaleway account logged into the [console](https://console.scaleway.com)
- [Owner](/identity-and-access-management/iam/concepts/#owner) status or [IAM permissions](/identity-and-access-management/iam/concepts/#permission) allowing you to perform actions in the intended Organization
- [Signed up to the private beta](https://www.scaleway.com/fr/betas/#distributed-data-lab) and received a confirmation email.
- Optionally, an [Object Storage bucket](/storage/object/how-to/create-a-bucket/)

## How to create a Distributed Data Lab cluster

1. Click **Data Lab** under **Managed Services** on the side menu.

2. Click **Create Data Lab cluster**. The creation wizard displays.

3. Complete the following steps in the wizard:
    - Choose an Apache Spark version from the drop-down menu.
    - Set up your worker nodes by selecting a configuration, and by indicating the desired number of Apache Spark nodes.
    - Optionally, choose an Object Storage bucket as your source of data and the place to store the output of your operations.
    - Enter a name for your Data Lab.
    - Optionally, add a description and/or tags for your Data Lab.
    - Verify the estimated cost.

4. Click **Create Data Lab cluster** to finish. You are directed to the Data Lab cluster overview page.

## How to connect to your Data Lab

1. Click **Data Lab** under **Managed Services** on the side menu. The Distributed Data Lab page displays.

2. Click the name of the Data Lab cluster you want to connect to. The cluster **Overview** page displays.

3. Click **Open Notebook** in the **Notebook** section. You are directed to the notebook login page.

4. Enter your [API secret key](/identity-and-access-management/iam/concepts/#api-key) when prompted for a password, then click **Log in**. You are directed to the notebook home screen.

## How to set up your Data Lab environment

1. From the notebook home screen, double-click on the environement desired.

2. Update the configuration cell of the file with your API access key and secret key, as shown below:

    ```json
    "spark.hadoop.fs.s3a.access.key": "[your-api-access-key]",
    "spark.hadoop.fs.s3a.secret.key": "[your-api-secret-key]",
    ```

    Your notebook environment is now ready to be used.

3. Follow the instructions contained in the `quickstart.ipynb` file to process a test batch of data.

## How to delete a Distributed Data Lab

<Message type="important">
  This action is irreversible and will permanently delete this Data Lab cluster and all its associated data.
</Message>

1. From the **Overview** tab of your Data Lab cluster, click the **Settings** tab, then select **Delete cluster**.

2. Enter **DELETE** in the confirmation pop-up to confirm your action.

3. Click **Delete Data Lab cluster**.
