---
meta:
  title: LLM Endpoints - Quickstart
  description: This page explains all the concepts related to Inference for LLMs
content:
  h1: LLM Endpoints - Quickstart
  paragraph: This page explains all the concepts related to Inference for LLMs
tags: 
categories:
  - ia
---

Scaleway LLM Inference service, a scalable and secure inference engine for machine learning models.

Scaleway LLM Inference is a fully managed service that allows you to run machine learning models in a production environment.
With Scaleway LLM Inference, you can easily deploy, manage, and scale your inference models without worrying about the underlying infrastructure.

Here are some of the key features of Scaleway Inference:

* **Easy deployment**: Deploy your inference models with just a few clicks. Scaleway Inference provides a simple and intuitive interface for uploading and deploying models.
* **Auto-scaling**: Scaleway LLM Inference automatically scales your inference instances based on the demand, ensuring that your models are always available and responsive.
* **Security**: Scaleway provides a secure environment for running your inference models. Our platform is built on top of a secure architecture, and we use state-of-the-art security.

## How to create a LLM Inference deployment


1. Navigate to the **AI & Data** section of the [Scaleway console](https://console.scaleway.com/), and select **LLM Inference** from the side menu to access the LLM Inference dashboard.
2. Click on **Create deployment** to launch the deployment creation wizard.
3. Provide the necessary information:
    - Select the desired model and the quantization to use for your deployment from the available options:
        - Llama-2-70b
        - Mixtral-8x7B-Instruct-v0.1
        - WizardLM-70b-V1.0
        <Message type="note">
          Some models may require acceptance of an end-user license agreement. If prompted, review the terms and conditions and accept the license accordingly.
        </Message>
    - Choose the geographical **region** for the deployment.
    - Specify the GPU Instance type to be used with your deployment.
4. Enter a **name** for the deployment, along with an optional description and tags to aid in organization.
5. Configure the **network** settings for the deployment:
    - Enable **Private Network** for secure communication and restricted availability within Private Networks. Choose an existing Private Network from the drop-down list, if applicable.
    - Enable **Public Network** to access resources via the public IP address of the GPU Instance.
    <Message type="important">
      - It is not possible to change network settings after the deployment creation.
      - Enabling both private and public networks will result in two distinct endpoints (public and private) for your deployment.
    </Message>
6. Click **Create deployment** to launch the deployment process. Once the deployment is ready, it will be listed among your deployments.

## How to access a LLM Inference deployment

1. Click **LLM Inference** in the **AI & Data** section of the side menu. The LLM Inference dashboard displays.
2. Click <Icon name="more" /> next to the deployment you want to edit. The deployment dashboard displays.
3. Click **Create token** in the **Deployment connection** section of the dashboard. The token creation wizard displays.
4. Fill in the [required information for token creation](/identity-and-access-management/iam/how-to/create-api-keys/) and click **Generate API key**.

<Message type="tip">
  You have full access to the management of your access tokens from the **Security** tab of your deployment.
</Message>

## How to retrieve information to interact with a deployment

1. Click **LLM Inference** in the **AI & Data** section of the side menu. The LLM Inference dashboard displays.
2. Click <Icon name="more" /> next to the deployment you want to edit. The deployment dashboard displays.
3. Click the **Inference** tab. Code examples in various languages displays. Copy and paste them in your code.

## How to delete a deployment

1. Click **LLM Inference** in the **AI & Data** section of the [Scaleway console](https://console.scaleway.com) side menu. A list of your deployments displays.
2. Choose a deployment either by clicking its name or selecting **More info** from the drop-down menu represented by the icon <Icon name="more" /> to access the deployment dashboard.
3. Click the **Settings** tab of your deployment to display additional settings.
4. Click **Delete deployment** to delete your deployment.
5. Type **DELETE** to confirm and click **Delete deployment** to delete your deployment. 
 
<Message type="important">
  Deleting a deployment is a permanent action and erases all its associated data.
</Message>
