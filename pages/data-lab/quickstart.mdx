---
title: Data Lab for Apache Spark™ - Quickstart
description: Get started with Scaleway Data Lab for Apache Spark™ quickly and efficiently.
tags: data lab apache spark notebook jupyter processing
dates:
  validation: 2025-12-09
  posted: 2024-07-10
---
import Requirements from '@macros/iam/requirements.mdx'

## Console overview
Follow this guided tour to discover how to navigate the console.
<GuideFlow src="https://app.guideflow.com/embed/9r2m9v2fzk"/>

Data Lab for Apache Spark™ is a product designed to assist data scientists and data engineers in performing calculations on a remotely managed Apache Spark™ infrastructure.

Scaleway provides dedicated node types for both the main node and the worker nodes of the cluster. The worker nodes are high-end machines built for intensive computations, featuring powerful CPUs/GPUs, and substantial RAM.

The main node, although capable of performing some local computations, primarily serves as a web interface for interacting with the Apache Spark™ cluster.

This documentation explains how to create a Data Lab for Apache Spark™ cluster, how to access its notebook environment and run the included demo file, and how to delete your cluster.

<Requirements />

- A Scaleway account logged into the [console](https://console.scaleway.com)
- [Owner](/iam/concepts/#owner) status or [IAM permissions](/iam/concepts/#permission) allowing you to perform actions in the intended Organization
- Created an [IAM API key](/iam/how-to/create-api-keys/)

## How to create a Data Lab for Apache Spark™ cluster

1. Click **Data Lab** under **Data & Analytics** on the side menu.

2. Click **Create Data Lab cluster**. The creation wizard displays.

3. Complete the following steps in the wizard:

    - Select a region for your cluster.
    - Choose an Apache Spark™ version from the drop-down menu.
    - Select the **DDL-PLAY2-MICRO** main nide type.
    - Select a **CPU** worker node configuration.
    - Enter the desired number of worker nodes.
    - Enter a name for your cluster.
    - Optionally, add a description and/or tags.
    - Verify the estimated cost.

4. Click **Create Data Lab cluster** to finish.

Once the cluster is created, you are directed to its **Overview** page.

## How to connect to your cluster's notebook

1. Click **Data Lab** under **Data & Analytics** on the side menu. The Data Lab for Apache Spark™ page displays.

2. Click the name of the Data Lab cluster you want to connect to. The cluster **Overview** page displays.

3. Click **Open Notebook** in the **Notebook** section. You are directed to the notebook login page.

4. Enter your [API secret key](/iam/concepts/#api-key) when prompted for a password, then click **Log in**. You are directed to the notebook home screen.

## How to run the demo file

Each Data Lab for Apache Spark™ comes with a default `DatalabDemo.ipynb` demonstration file for testing purposes. This file contains a preconfigured notebook environment that requires no modification to run.

Execute the cells in order to perform pre-determined operations on a dummy data set representative of real life use cases and workloads to assess the performance of your cluster.

<Message type="tip">
The demo file also contains a set of examples to configure and extend your Apache Spark™ configuration.
</Message>

## How to delete a Data Lab for Apache Spark™

<Message type="important">
  This action is irreversible and will permanently delete this Data Lab cluster and all its associated data.
</Message>

1. From the **Overview** tab of your Data Lab cluster, click the **Settings** tab, then select **Delete cluster**.

2. Enter **DELETE** in the confirmation pop-up to confirm your action.

3. Click **Delete Data Lab cluster**.
