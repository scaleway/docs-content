---
meta:
  title: How to monitor an LLM Inference deployment
  description: This page explains how to monitor an LLM Inference deployment
content:
  h1: How to monitor an LLM Inference deployment
  paragraph: This page explains how to monitor an LLM Inference deployment
tags: ai monitoring
dates:
  validation: 2024-03-06
  posted: 2024-03-06
categories:
  - ai-data
---

This documentation page shows you how to monitor your LLM Inference deployment with [Cockpit](/observability/cockpit/quickstart/).

<Macro id="cockpit-update-dashboards" />

<Macro id="requirements" />

  - A Scaleway account logged into the [console](https://console.scaleway.com)
  - An [LLM Inference deployment](/ai-data/llm-inference/quickstart/)


## How to monitor your LLM dashboard

1. Click **LLM Inference** in the **AI & Data** section of the [Scaleway console](https://console.scaleway.com) side menu. A list of your deployments displays.
2. Click a deployment name or <Icon name="more" /> > **More info** to access the deployment dashboard.
3. Click the **Monitoring** tab of your deployment. The Cockpit overview displays.
4. Click **Open Grafana metrics dashboard** to open your Cockpit's Grafana interface.
5. Authenticate with your [Grafana credentials](/observability/cockpit/how-to/retrieve-grafana-credentials/). The Grafana dashboard displays.
6. Select your LLM Inference dashboard from the [list of your managed dashboards](/observability/cockpit/how-to/access-grafana-and-managed-dashboards/) to visualize your metrics.