---
meta:
  title: Deploying the MLX array framework for Apple silicon
  description: This page provides information how to install and use the MLX array framework on Scaleway's Apple silicon as a Service.
content:
  h1: Deploying the MLX array framework for Apple silicon
  paragraph: This page provides information how to install and use the MLX array framework on Scaleway's Apple silicon as a Service.
categories:
  - apple-silicon
tags: apple-silicon mlx framework apple mac-mini
dates:
  validation: 2023-12-15
  posted: 2023-12-15
---

MLX, an array framework designed for effective and versatile machine learning on Apple silicon, was developed by Appleâ€™s machine learning research team.
MLX is developed by machine learning researchers focusing on catering to their peers, emphasizing a balance between user-friendliness and efficiency in model training and deployment. The framework boasts a deliberately straightforward design that facilitates seamless extension and enhancement by researchers, fostering the swift exploration of innovative ideas.

Regarding APIs, the Python interface of MLX closely emulates NumPy, incorporating a few distinctive features. MLX also offers a robust C++ API designed closely with its Python counterpart.

Taking inspiration from frameworks such as PyTorch, Jax, and ArrayFire, MLX takes a unique path with its unified memory model. Within MLX, arrays reside in shared memory, allowing operations on MLX arrays across diverse supported devices without data copies. This approach marks a notable distinction from other frameworks.

Key features of MLX include:

* **Familiar APIs:** MLX provides a Python API closely aligned with NumPy, complemented by a comprehensive C++ API mirroring its Python counterpart. Higher-level packages like `mlx.nn` and `mlx.optimizers` closely follow PyTorch, simplifying the construction of intricate models.
* **Composable Function Transformations:** MLX facilitates composable function transformations, supporting automatic differentiation, vectorization, and optimization of computation graphs.
* **Lazy Computation:** Within MLX, computations adopt a lazy approach, only materializing arrays when needed.
* **Dynamic Graph Construction:** MLX dynamically constructs computation graphs, allowing for shape changes in function arguments without triggering sluggish compilations. Debugging remains straightforward and intuitive.
* **Multi-device Support:** MLX operations are versatile and capable of running on supported devices, currently spanning the CPU and GPU.
* **Unified Memory:** A standout distinction lies in MLX's unified memory model. Arrays within MLX reside in shared memory, permitting operations on arrays across different device types without necessitating data transfers.

<Macro id="requirements" />

- A Scaleway account logged into the [console](https://console.scaleway.com)
- [Owner](/identity-and-access-management/iam/concepts/#owner) status or [IAM permissions](/identity-and-access-management/iam/concepts/#permission) allowing you to perform actions in the intended Organization
- [Created a Mac mini](/bare-metal/apple-silicon/how-to/create-mac-mini/)
- Installed native Python >= 3.8 on the Mac (preinstalled by default)

<Message type="note">
    MLX is only available on devices running MacOS >= 13.3.
</Message>

## Installing MLX

To install MLX from PyPI for use on your Apple silicon computer, execute the following command from a terminal or the SSH shell:

```bash
pip3 install mlx
```

## Getting started with MLX

### MLX basics

To work with MLX, begin by importing ``mlx.core`` and creating an [`array`](https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.core.array.html#mlx.core.array):

```python
import mlx.core as mx
a = mx.array([1, 2, 3, 4])
print(a.shape)  # Output: [4]
print(a.dtype)  # Output: int32
b = mx.array([1.0, 2.0, 3.0, 4.0])
print(b.dtype)  # Output: float32
```

Operations in MLX are lazy, meaning the outputs are not computed until necessary. To enforce array evaluation, use [`eval`](https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.core.eval.html#mlx.core.eval). Arrays may also be automatically evaluated in specific scenarios, such as inspecting a scalar with [`array.item`](https://ml-explore.github.io/mlx/build/html/quick_start.html#:~:text=a%20scalar%20with-,array.item(),-%2C%20printing%20an%20array), printing an array, or converting an array from [`array`](https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.core.array.html#mlx.core.array) to [`numpy.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray).

```python
c = a + b  # c not yet evaluated
mx.eval(c)  # evaluates c
c = a + b
print(c)   # Also evaluates c
# Output: array([2, 4, 6, 8], dtype=float32)
c = a + b
import numpy as np
np.array(c)   # Also evaluates c
# Output: array([2., 4., 6., 8.], dtype=float32)
```

### Function and graph transformations

MLX supports standard function transformations like [`grad`](https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.core.grad.html#mlx.core.grad) and [`vmap`](https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.core.vmap.html#mlx.core.vmap), allowing arbitrary compositions. For instance, ``grad(vmap(grad(fn)))`` is a valid composition.

```python
x = mx.array(0.0)
print(mx.sin(x))
# Output: array(0, dtype=float32)
print(mx.grad(mx.sin)(x))
# Output: array(1, dtype=float32)
print(mx.grad(mx.grad(mx.sin))(x))
# Output: array(-0, dtype=float32)
```

Other gradient transformations include [`vjp`](https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.core.vjp.html#mlx.core.vjp) for vector-Jacobian products and [`jvp`](https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.core.jvp.html#mlx.core.jvp) for Jacobian-vector products.

Efficiently compute both a function's output and gradient with respect to the function's input using [`value_and_grad`](https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.core.value_and_grad.html#mlx.core.value_and_grad).

## Going further

- For comprehensive MLX documentation, refer to the [official MLX documenation](https://ml-explore.github.io/mlx/build/html/index.html).
- The [MLX examples repository](https://github.com/ml-explore/mlx-examples) hosts a diverse collection of examples, including:
    - Training a [Transformer language model](https://github.com/ml-explore/mlx-examples/tree/main/transformer_lm).
    - Large-scale text generation using [LLaMA](https://github.com/ml-explore/mlx-examples/tree/main/llms/llama) and subsequent finetuning with [LoRA](https://github.com/ml-explore/mlx-examples/tree/main/lora).
    - Image generation employing [Stable Diffusion](https://github.com/ml-explore/mlx-examples/tree/main/stable_diffusion).
    - Speech recognition utilizing [OpenAI's Whisper](https://github.com/ml-explore/mlx-examples/tree/main/whisper).